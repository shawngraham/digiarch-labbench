{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be675a6-7993-4a95-9221-ddcf05cb082e",
   "metadata": {},
   "source": [
    "## A Quick Intro to Sonification\n",
    "\n",
    "First we'll get the python packages we'll need, and then I'll show you a complete script that turns a list of notes into a .mid (midi) file you can hear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10687991-060f-40d7-9c67-687bfdc17768",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install miditime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75cf429-7616-4d2a-a635-dc96ccd783ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A demo script that creates two notes\n",
    "import miditime\n",
    "from datetime import datetime\n",
    "from miditime.MIDITime import MIDITime\n",
    "\n",
    "# Instantiate the class with a tempo (120bpm is the default) and an output file destination.\n",
    "mymidi = MIDITime(120, 'myfile.mid')\n",
    "\n",
    "# Create a list of notes. Each note is a list: [time, pitch, attack, duration]\n",
    "midinotes = [\n",
    "    [0, 60, 200, 3],  #At 0 beats (the start), Middle C with attack 200, for 3 beats\n",
    "    [10, 61, 200, 4]  #At 10 beats (12 seconds from start), C#5 with attack 200, for 4 beats\n",
    "]\n",
    "\n",
    "# Add a track with those notes\n",
    "mymidi.add_track(midinotes)\n",
    "\n",
    "# Output the .mid file\n",
    "mymidi.save_midi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f5d582-c870-49ca-ac80-71fa1928b403",
   "metadata": {},
   "source": [
    "Your music file - with just two notes! - is now in your folder for this week. Find the folder using your Finder or Windows Explorer. Double-click it on `myfile.mid`. Whatever program you use for music on your own computer _should_ be able to play it. A mid file is not, in itself, music (in the sense that an mp3 is a compressed representation of the music. It is more akin to the digital representation of a score that the computer plays with a default instrument (often, a piano). Typically, it sounds pretty tinkly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a060e-8df3-4d04-9906-9e210f2158df",
   "metadata": {},
   "source": [
    "## Writing music\n",
    "\n",
    "Play with that script now, and add more notes. Try something simple - the notes for ‘Baa Baa Black Sheep’ are:\n",
    "\n",
    "D, D, A, A, B, B, B, B, A\n",
    "Baa, Baa, black, sheep, have, you, any, wool?\n",
    "\n",
    "Middle C is '60' - [this chart](https://web.archive.org/web/20171211192102/http://www.electronics.dit.ie/staff/tscarff/Music_technology/midi/midi_note_numbers_for_octaves.htm) shows the numerical representation of each note on the 88 key keyboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a5620-00fc-45be-a3ea-070b3adc1c65",
   "metadata": {},
   "source": [
    "## Representing Data\n",
    "\n",
    "Let's represent some data\n",
    "\n",
    "The miditime package is written with time series data in mind:\n",
    "\n",
    "```json\n",
    "my_data = [\n",
    "    {'event_date': <datetime object>, 'magnitude': 3.4},\n",
    "    {'event_date': <datetime object>, 'magnitude': 3.2},\n",
    "    {'event_date': <datetime object>, 'magnitude': 3.6},\n",
    "    {'event_date': <datetime object>, 'magnitude': 3.0},\n",
    "    {'event_date': <datetime object>, 'magnitude': 5.6},\n",
    "    {'event_date': <datetime object>, 'magnitude': 4.0}\n",
    "]```\n",
    "\n",
    "Here we're dealing with earthquake data, and it's in json notation. The `datetime object` is a particular way of formatting the date: `datetime(1753,6,8)` would be June 8 1753. Following the example that miditime provides, we can build up a sonification of this earthquake data like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceaaaef-c78d-4fdc-9726-65e72bb87027",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we'll define 'mymidi' which will hold all of our sonified information.\n",
    "# tempo (120bpm is the default), an output file destination, \n",
    "# the number of seconds you want to represent a year in the final song (default is 5 sec/year),\n",
    "# the base octave (C5 is middle C, so the default is 5, \n",
    "# and how many octaves you want your output to range over (default is 1)\n",
    "\n",
    "mymidi = MIDITime(120, 'second-example.mid', 0.1, 5, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debbdce3-6297-4e01-a0ce-d27504caf84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = [\n",
    "    {'event_date': datetime(1792,6,8), 'magnitude': 3.4},\n",
    "    {'event_date': datetime(1800,3,4), 'magnitude': 3.2},\n",
    "    {'event_date': datetime(1810,1,16), 'magnitude': 3.6},\n",
    "    {'event_date': datetime(1812,8,23), 'magnitude': 3.0},\n",
    "    {'event_date': datetime(1813,10,10), 'magnitude': 5.6},\n",
    "    {'event_date': datetime(1824,1,5), 'magnitude': 4.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7735cc-68b3-4dd7-8f4a-bafc10216694",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now we convert those dates into an integer. Oddly enough, this is done by defining time since 'epoch'. This epoch date is Jan 1 1970. The reasons why have to do with the evolution of unix. For dates before 1970, we end up with a negative number, but this is not a problem.\n",
    "\n",
    "First we convert the date so that it is expressed with reference to the epoch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9797e-ea95-408e-8765-9575f0633245",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_epoched = [{'days_since_epoch': mymidi.days_since_epoch(d['event_date']), 'magnitude': d['magnitude']} for d in my_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aeda4f-269a-4717-b689-54a1ae9c1453",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_epoched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f06d829-45d8-49cc-b448-a51d7237dbdc",
   "metadata": {},
   "source": [
    "And then we convert that integer into something reasonable for music. Per the miditime package:\n",
    "\n",
    "> Convert your integer date/time to something reasonable for a song. For example, at 120 beats per minute, you'll need to scale the data down a lot to avoid a very long song if your data spans years. This uses the seconds_per_year attribute you set at the top, so if your date is converted to something other than days you may need to do your own conversion. But if your dataset spans years and your dates are in days (with fractions is fine), use the beat() helper method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81307a27-2987-40ff-a86c-697911d2d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_timed = [{'beat': mymidi.beat(d['days_since_epoch']), 'magnitude': d['magnitude']} for d in my_data_epoched]\n",
    "my_data_timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11338e72-8a2e-4fc4-861d-933c7a1d908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the earliest date in your series so you can set that to 0 in the MIDI:\n",
    "start_time = my_data_timed[0]['beat']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48532c7-bd3a-481a-80bc-3ae4e7fbe608",
   "metadata": {},
   "source": [
    "Finally, we define how the data get mapped against the pitch. If the data were percentages ranging from 0.01 (ie 1%) to 0.99 (99%), we would scale_pct below between 0 and 1. If you weren’t dealing with percentages, you’d use your lowest value and your highest value (say if for instance your data were counts of pottery over time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3475aae0-57f2-478c-bfb8-c34c3abb4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some functions to scale your other variable (magnitude in our case) \n",
    "# to match your desired mode/key and octave range. \n",
    "# There are helper methods to assist this scaling\n",
    "# You can choose a linear or logarithmic scale.\n",
    "\n",
    "def mag_to_pitch_tuned(magnitude):\n",
    "    # Where does this data point sit in the domain of your data? \n",
    "    #(I.E. the min magnitude is 3, the max in 5.6). In this case the optional 'True' \n",
    "    # means the scale is reversed, so the highest value will return the lowest percentage.\n",
    "    scale_pct = mymidi.linear_scale_pct(3, 5.7, magnitude)\n",
    "\n",
    "    # Another option: Linear scale, reverse order\n",
    "    # scale_pct = mymidi.linear_scale_pct(3, 5.7, magnitude, True)\n",
    "\n",
    "    # Another option: Logarithmic scale, reverse order\n",
    "    # scale_pct = mymidi.log_scale_pct(3, 5.7, magnitude, True)\n",
    "\n",
    "    # Pick a range of notes. This allows you to play in a key.\n",
    "    c_major = ['C', 'D', 'E', 'F', 'G', 'A', 'B']\n",
    "\n",
    "    #Find the note that matches your data point\n",
    "    note = mymidi.scale_to_note(scale_pct, c_major)\n",
    "\n",
    "    #Translate that note to a MIDI pitch\n",
    "    midi_pitch = mymidi.note_to_midi_pitch(note)\n",
    "\n",
    "    return midi_pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d25fd00-8d74-42c2-b829-26f4a40aeecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now build the note list\n",
    "note_list = []\n",
    "\n",
    "for d in my_data_timed:\n",
    "    note_list.append([\n",
    "        d['beat'] - start_time,\n",
    "        mag_to_pitch_tuned(d['magnitude']),\n",
    "        100,  # velocity\n",
    "        1  # duration, in beats\n",
    "    ])\n",
    "\n",
    "# show the result\n",
    "note_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b510e-d9c0-4b63-b885-cd5b2e286544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a track with those notes\n",
    "mymidi.add_track(note_list)\n",
    "\n",
    "# Output the .mid file\n",
    "mymidi.save_midi()\n",
    "\n",
    "# incidentally, to add more tracks, you'd just repeat the process to build more note_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b496c74a-8267-423b-8707-e344e879dc21",
   "metadata": {},
   "source": [
    "Find, then open `second-example.mid` using your computer's music program. How does it sound? It might not be 'music' - but that's not the point.\n",
    "\n",
    "Try feeding actual archaeological data that you've retrieved from other exercises into this program. Save each result under a different name; you can then begin to mix the data as unique voices using something like GarageBand.\n",
    "\n",
    "For other approaches to sonification, [please see this tutorial](https://programminghistorian.org/en/lessons/sonification). For another creative use of sonification by an undergraduate, see [Daniel Ruten's Sonic Word Clouds](https://programminghistorian.org/posts/sonic-word-clouds).\n",
    "\n",
    "Cool, eh?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
