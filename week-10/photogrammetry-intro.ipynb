{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c987f3ce-87ca-4daf-acef-81bfc7849f2b",
   "metadata": {},
   "source": [
    "# 3d models from photos and videos\n",
    "\n",
    "There are any number of software packages that will take your photos and work out how the images overlap to combine into a 3d model. On your desktop, Windows users might want to look up 3DF Zephyr Free; Mac users can try Meshroom (but it's not for the faint of heart [link](https://alicevision.org/#meshroom))\n",
    "\n",
    "A commercial service that will do the computation for you is [https://poly.cam/](https://poly.cam/); they have a free tier but can be aggressive with their pop-ups that try to nudge you into paying for things. Close the pop-ups, continue with the free tier.\n",
    "\n",
    "In this notebook, I have gently modified the demonstration process of Florent Poux, who wrote: [https://towardsdatascience.com/master-the-3d-reconstruction-process-step-by-step-guide/](https://towardsdatascience.com/master-the-3d-reconstruction-process-step-by-step-guide/). \n",
    "\n",
    "**The goal for this notebook is to show you how the process works**. Remember those pictures you took way back in week 3? We'll talk about those in a moment once you've seen how the magic works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e90db4-4f36-4c34-9a8b-d72ba9df5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preliminaries\n",
    "# we need a library meant to manipulate image data\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74525c74-6432-410c-96e4-e47754dac03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's grab a fun set of test images\n",
    "## Sceaux Castle in France!\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Create the images directory\n",
    "os.makedirs('images', exist_ok=True)\n",
    "\n",
    "# Base URL pattern\n",
    "base_url = 'https://github.com/openMVG/ImageDataset_SceauxCastle/blob/master/images/100_71{:02d}.JPG?raw=true'\n",
    "\n",
    "# Loop through 100_7100.JPG to 100_7110.JPG\n",
    "for i in range(11):  # 0 to 10 inclusive\n",
    "    url = base_url.format(i)\n",
    "    filename = f'images/100_71{i:02d}.jpg'\n",
    "    \n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download image 100_71{i:02d}.jpg: {e}\")\n",
    "\n",
    "print(\"Download process completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a5b69e-3e81-479a-b22a-b0e03f44cda0",
   "metadata": {},
   "source": [
    "Now we need to find features in the images that can be used to build up our image, that we can track from one image to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b797d-a7e9-4a6f-a482-b7e83c0d19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% SECTION 1: Natural Feature Extraction\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_features(image_path, feature_method='sift', max_features=2000):\n",
    "    \"\"\"\n",
    "    Extract features from an image using different methods.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the image in color and convert to grayscale\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read image at {image_path}\")\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Initialize feature detector based on method\n",
    "    if feature_method.lower() == 'sift':\n",
    "        detector = cv2.SIFT_create(nfeatures=max_features)\n",
    "    elif feature_method.lower() == 'surf':\n",
    "        # Note: SURF is patented and may not be available in all OpenCV distributions\n",
    "        detector = cv2.xfeatures2d.SURF_create(400)  # Adjust threshold as needed\n",
    "    elif feature_method.lower() == 'orb':\n",
    "        detector = cv2.ORB_create(nfeatures=max_features)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported feature method: {feature_method}\")\n",
    "    \n",
    "    # Detect and compute keypoints and descriptors\n",
    "    keypoints, descriptors = detector.detectAndCompute(gray, None)\n",
    "    \n",
    "    # Create visualization\n",
    "    img_with_features = cv2.drawKeypoints(\n",
    "        img, keypoints, None, \n",
    "        flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    "    )\n",
    "    \n",
    "    print(f\"Extracted {len(keypoints)} {feature_method.upper()} features\")\n",
    "    \n",
    "    return keypoints, descriptors, img_with_features\n",
    "\n",
    "# now let's identify features in one of the images\n",
    "image_path = \"images/100_7100.JPG\"  \n",
    "\n",
    "# Extract features with different methods\n",
    "kp_sift, desc_sift, vis_sift = extract_features(image_path, 'sift')\n",
    "kp_orb, desc_orb, vis_orb = extract_features(image_path, 'orb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b590d834-ee98-4bf9-8d29-9f320fb5b70f",
   "metadata": {},
   "source": [
    "Ok, but what does that mean? These are two different algorithms for identifying keypoints in an image. In an actual workflow we might try a variety of algorithms and then decide which one to move forward with. Let's see what difference the two algorithms make:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf78f97-7ebf-4ca3-9b07-713b7935572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "plt.figure(figsize=(12, 6))\n",
    "    \n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(f'SIFT Features ({len(kp_sift)})')\n",
    "plt.imshow(cv2.cvtColor(vis_sift, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(f'ORB Features ({len(kp_orb)})')\n",
    "plt.imshow(cv2.cvtColor(vis_orb, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a81b9d-8fbb-4ba3-80c2-21f8484f9f47",
   "metadata": {},
   "source": [
    "Now we do that for every image, and then identify pairs of matching points across images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca213a94-905e-4aee-8a49-0167a27627c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% SECTION 2: Feature Matching\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def match_features(descriptors1, descriptors2, method='flann', ratio_thresh=0.75):\n",
    "    \"\"\"\n",
    "    Match features between two images using different methods.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert descriptors to appropriate type if needed\n",
    "    if descriptors1 is None or descriptors2 is None:\n",
    "        return []\n",
    "    \n",
    "    if method.lower() == 'flann':\n",
    "        # FLANN parameters\n",
    "        if descriptors1.dtype != np.float32:\n",
    "            descriptors1 = np.float32(descriptors1)\n",
    "        if descriptors2.dtype != np.float32:\n",
    "            descriptors2 = np.float32(descriptors2)\n",
    "            \n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "        search_params = dict(checks=50)  # Higher values = more accurate but slower\n",
    "        \n",
    "        flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n",
    "    else:  # Brute Force\n",
    "        # For ORB descriptors\n",
    "        if descriptors1.dtype == np.uint8:\n",
    "            bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "        else:  # For SIFT and SURF descriptors\n",
    "            bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "        \n",
    "        matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "    \n",
    "    # Apply Lowe's ratio test\n",
    "    good_matches = []\n",
    "    for match in matches:\n",
    "        if len(match) == 2:  # Sometimes fewer than 2 matches are returned\n",
    "            m, n = match\n",
    "            if m.distance < ratio_thresh * n.distance:\n",
    "                good_matches.append(m)\n",
    "    \n",
    "    return good_matches\n",
    "\n",
    "def visualize_matches(img1, kp1, img2, kp2, matches, max_display=100):\n",
    "    \"\"\"\n",
    "    Create a visualization of feature matches between two images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Limit the number of matches to display\n",
    "    matches_to_draw = matches[:min(max_display, len(matches))]\n",
    "    \n",
    "    # Create match visualization\n",
    "    match_img = cv2.drawMatches(\n",
    "        img1, kp1, img2, kp2, matches_to_draw, None,\n",
    "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "    )\n",
    "    \n",
    "    return match_img\n",
    "\n",
    "# Load two images\n",
    "img1_path = \"images/100_7100.jpg\"  \n",
    "img2_path = \"images/100_7101.jpg\"\n",
    "    \n",
    "# Extract features using SIFT (or your preferred method)\n",
    "kp1, desc1, _ = extract_features(img1_path, 'sift')\n",
    "kp2, desc2, _ = extract_features(img2_path, 'sift')\n",
    "    \n",
    "# Match features\n",
    "good_matches = match_features(desc1, desc2, method='flann')\n",
    "    \n",
    "print(f\"Found {len(good_matches)} good matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd251c-0bf7-4490-8c0f-e94fdd5dd6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize matches\n",
    "img1 = cv2.imread(img1_path)\n",
    "img2 = cv2.imread(img2_path)\n",
    "match_visualization = visualize_matches(img1, kp1, img2, kp2, good_matches)\n",
    "    \n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cv2.cvtColor(match_visualization, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f\"Feature Matches: {len(good_matches)}\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83968fa5-6592-41f4-afbb-c47a273f3860",
   "metadata": {},
   "source": [
    "Most of those look good, eh? But you can see that there are some bad matches. Ideally, we'd spend some time pruning those ones out. Do you see which ones I mean?\n",
    "\n",
    "The next block uses this information to work out how much one image has moved (or rather, where the camera was for picture one versus picture two) so that the relative positioning of these points can be determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df5aeb-728d-4675-a5f4-7e8cd888064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% SECTION 3: Structure from Motion \n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def estimate_pose(kp1, kp2, matches, K, method=cv2.RANSAC, prob=0.999, threshold=1.0):\n",
    "    \"\"\"\n",
    "    Estimate the relative pose between two cameras using matched features.\n",
    "    This function now correctly filters matches using the essential matrix mask.\n",
    "    \"\"\"\n",
    "    # Extract matched keypoints\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    \n",
    "    # Estimate the Essential Matrix using RANSAC\n",
    "    E, mask = cv2.findEssentialMat(pts1, pts2, K, method=method, prob=prob, threshold=threshold)\n",
    "    \n",
    "    if E is None:\n",
    "        raise ValueError(\"Could not compute the Essential Matrix. Check your matches and K matrix.\")\n",
    "        \n",
    "    # Recover the relative rotation (R) and translation (t) from the Essential Matrix\n",
    "    _, R, t, mask = cv2.recoverPose(E, pts1, pts2, K, mask=mask)\n",
    "    \n",
    "    # Filter matches using the RANSAC mask to get only the inliers\n",
    "    inlier_matches = [m for i, m in enumerate(matches) if mask[i]]\n",
    "    \n",
    "    print(f\"Estimated pose with {len(inlier_matches)} inliers out of {len(matches)} initial matches.\")\n",
    "    \n",
    "    return R, t, inlier_matches\n",
    "\n",
    "def triangulate_points(kp1, kp2, matches, K, R1, t1, R2, t2):\n",
    "    \"\"\"\n",
    "    Triangulate 3D points from two views using only the inlier matches.\n",
    "    \"\"\"\n",
    "    # Extract matched keypoints from the inlier matches\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches])\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches])\n",
    "    \n",
    "    # Create projection matrices for both cameras\n",
    "    P1 = K @ np.hstack((R1, t1))\n",
    "    P2 = K @ np.hstack((R2, t2))\n",
    "    \n",
    "    # Triangulate 3D points\n",
    "    points_4d_hom = cv2.triangulatePoints(P1, P2, pts1.T, pts2.T)\n",
    "    \n",
    "    # Convert from homogeneous to 3D coordinates by dividing by the 4th dimension\n",
    "    points_3d = (points_4d_hom[:3] / points_4d_hom[3]).T\n",
    "    \n",
    "    return points_3d\n",
    "\n",
    "def visualize_reconstruction(points_3d, R1, t1, R2, t2, K):\n",
    "    \"\"\"\n",
    "    Visualize 3D points and camera frustums for a more intuitive view.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot the 3D point cloud\n",
    "    ax.scatter(points_3d[:, 0], points_3d[:, 1], points_3d[:, 2], c='b', marker='.', s=5, label='Reconstructed Points')\n",
    "    \n",
    "    # Helper function to plot a camera frustum\n",
    "    def plot_camera_frustum(R, t, K, color, scale=0.5):\n",
    "        # Camera center\n",
    "        cam_center = -R.T @ t\n",
    "        ax.scatter(cam_center[0], cam_center[1], cam_center[2], c=color, s=100, marker='o')\n",
    "\n",
    "        # Frustum corners in image space (pixels)\n",
    "        h, w = K[1, 2] * 2, K[0, 2] * 2 # Approximate image dimensions from K\n",
    "        corners = np.array([\n",
    "            [0, 0, 1],\n",
    "            [w, 0, 1],\n",
    "            [w, h, 1],\n",
    "            [0, h, 1],\n",
    "            [0, 0, 1] # Close the loop\n",
    "        ])\n",
    "\n",
    "        # Invert K to back-project points to normalized camera coordinates\n",
    "        inv_K = np.linalg.inv(K)\n",
    "        frustum_corners_cam = inv_K @ corners.T\n",
    "        \n",
    "        # Transform frustum from camera coordinates to world coordinates\n",
    "        frustum_world = (R.T @ (frustum_corners_cam * scale)) - (R.T @ t)\n",
    "\n",
    "        # Plot the frustum lines\n",
    "        ax.plot(frustum_world[0, :], frustum_world[1, :], frustum_world[2, :], color=color) # Base\n",
    "        for i in range(4): # Pyramidal lines\n",
    "            ax.plot([cam_center[0,0], frustum_world[0, i]], \n",
    "                    [cam_center[1,0], frustum_world[1, i]], \n",
    "                    [cam_center[2,0], frustum_world[2, i]], color=color)\n",
    "\n",
    "    # Plot the first camera (at the origin)\n",
    "    plot_camera_frustum(R1, t1, K, 'red')\n",
    "    \n",
    "    # Plot the second camera\n",
    "    plot_camera_frustum(R2, t2, K, 'green')\n",
    "    \n",
    "    ax.set_title('3D Reconstruction: Points and Camera Frustums')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Auto-scale axes to be equal\n",
    "    max_range = np.array([points_3d[:,0].max()-points_3d[:,0].min(), \n",
    "                          points_3d[:,1].max()-points_3d[:,1].min(), \n",
    "                          points_3d[:,2].max()-points_3d[:,2].min()]).max()\n",
    "    mid_x = (points_3d[:,0].max()+points_3d[:,0].min()) * 0.5\n",
    "    mid_y = (points_3d[:,1].max()+points_3d[:,1].min()) * 0.5\n",
    "    mid_z = (points_3d[:,2].max()+points_3d[:,2].min()) * 0.5\n",
    "    ax.set_xlim(mid_x - max_range, mid_x + max_range)\n",
    "    ax.set_ylim(mid_y - max_range, mid_y + max_range)\n",
    "    ax.set_zlim(mid_z - max_range, mid_z + max_range)\n",
    "    \n",
    "    ax.view_init(elev=-70, azim=-90) # Better viewing angle\n",
    "    plt.show()\n",
    "\n",
    "# The Camera Intrinsic Matrix (K)\n",
    "# Think of the K matrix as the camera's \"fingerprint.\" \n",
    "# It's unique to a specific camera and lens combination and it allows us to translate \n",
    "# between the 2D pixel coordinates of an image and 3D rays in space. It looks like this:\n",
    "#     [ [fx,  0, cx],\n",
    "# K =  [ 0, fy, cy],\n",
    "#      [ 0,  0,  1] ]\n",
    "#    fx and fy are the focal lengths of the camera, but measured in pixel units. \n",
    "# They determine the \"zoom\" of the camera. cx and cy are the coordinates of the principal point, \n",
    "# which is the point where the central ray from the lens intersects the image sensor. \n",
    "# This is usually very close to the center of the image.\n",
    "\n",
    "# For our purpose, the K below is a reasonable guess for a standard camera. \n",
    "# In a high-precision application, you would determine these values precisely \n",
    "# through a camera calibration process (e.g., taking pictures of a checkerboard). \n",
    "# Having an accurate K matrix is crucial for an accurate 3D reconstruction.\n",
    "\n",
    "#img_height, img_width = 480, 640\n",
    "#K = np.array([\n",
    "#    [800, 0, img_width / 2],  # fx, 0, cx\n",
    "#    [0, 800, img_height / 2], # 0, fy, cy\n",
    "#    [0, 0, 1]\n",
    "#])\n",
    "\n",
    "# but actually, the dataset K values are available at https://github.com/openMVG/ImageDataset_SceauxCastle/blob/master/images/K.txt\n",
    "K = np.array([\n",
    "    [2905.88, 0, 1416],\n",
    "    [0, 2905.88, 1064],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "# Define the first camera's pose as the world origin\n",
    "R1 = np.eye(3)\n",
    "t1 = np.zeros((3, 1))\n",
    "\n",
    "# Load images, extract features, and match as in previous sections\n",
    "img1_path = \"images/100_7100.jpg\"  # \n",
    "img2_path = \"images/100_7101.jpg\"\n",
    "\n",
    "img1 = cv2.imread(img1_path)\n",
    "img2 = cv2.imread(img2_path)\n",
    "\n",
    "# Extract features and match them (as in previous sections)\n",
    "kp1, desc1, _ = extract_features(img1_path, 'sift')\n",
    "kp2, desc2, _ = extract_features(img2_path, 'sift')\n",
    "initial_matches = match_features(desc1, desc2, method='flann')\n",
    "\n",
    "# Estimate pose of the second camera and get geometrically verified inlier matches\n",
    "R2, t2, inlier_matches = estimate_pose(kp1, kp2, initial_matches, K)\n",
    "\n",
    "# Triangulate 3D points using ONLY the inlier matches\n",
    "points_3d = triangulate_points(kp1, kp2, inlier_matches, K, R1, t1, R2, t2)\n",
    "\n",
    "# Visualize the final 3D point cloud and camera poses\n",
    "visualize_reconstruction(points_3d, R1, t1, R2, t2, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67be647a-ec5f-42b0-9a43-3c8a3470bf53",
   "metadata": {},
   "source": [
    "So ... doesn't look like a 3d model, eh? What we're seeing here is that the code has worked out the relative positioning of the two cameras for the two different images, and we can see the cloud of points that would anchor the eventual model. Since we also know where these points are in the original image, the image data can then be draped over top of the points and voilÃ , a 3d model from 2d photographs. We just need to tie it all together now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6841a7-678c-405a-af25-8e7733e31b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% SECTION 4: Full Sequential Structure from Motion Pipeline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "\n",
    "\n",
    "# --- Re-use previously defined functions ---\n",
    "# extract_features, match_features, and the enhanced visualize_reconstruction \n",
    "# and other helper functions are defined in the earlier code blocks\n",
    "\n",
    "\n",
    "# --- Configuration and Setup ---\n",
    "IMAGE_DIR = \"images\"\n",
    "image_files = sorted([os.path.join(IMAGE_DIR, f) for f in os.listdir(IMAGE_DIR) if f.endswith('.jpg')])\n",
    "K = np.array([[2905.88, 0, 1416], [0, 2905.88, 1064], [0, 0, 1]])\n",
    "\n",
    "# --- Data storage ---\n",
    "all_points_3d = np.zeros((0, 3))\n",
    "map_point_descriptors = np.zeros((0, 128), dtype=np.float32)\n",
    "keyframes = []\n",
    "\n",
    "# --- 1. Initial Reconstruction (Bootstrap) ---\n",
    "print(\"Step 1: Initializing with the first two views...\")\n",
    "img1_path, img2_path = image_files[0], image_files[1]\n",
    "kp1, desc1, _ = extract_features(img1_path, 'sift')\n",
    "kp2, desc2, _ = extract_features(img2_path, 'sift')\n",
    "initial_matches = match_features(desc1, desc2, 'flann')\n",
    "R_init, t_init, inlier_matches = estimate_pose(kp1, kp2, initial_matches, K)\n",
    "if R_init is None: raise ConnectionError(\"Initial pose estimation failed.\")\n",
    "\n",
    "R1, t1 = np.eye(3), np.zeros((3, 1))\n",
    "initial_points_3d = triangulate_points(kp1, kp2, inlier_matches, K, R1, t1, R_init, t_init)\n",
    "all_points_3d = np.vstack((all_points_3d, initial_points_3d))\n",
    "\n",
    "# Populate the initial map descriptors using the keypoints from the first image\n",
    "for match in inlier_matches:\n",
    "    map_point_descriptors = np.vstack((map_point_descriptors, desc1[match.queryIdx]))\n",
    "\n",
    "# Create keyframes (now only storing pose and descriptors for map expansion)\n",
    "first_keyframe = {'id': 0, 'pose': (R1, t1), 'keypoints': kp1, 'descriptors': desc1}\n",
    "second_keyframe = {'id': 1, 'pose': (R_init, t_init), 'keypoints': kp2, 'descriptors': desc2}\n",
    "keyframes.extend([first_keyframe, second_keyframe])\n",
    "\n",
    "initial_baseline = np.linalg.norm(t_init)\n",
    "print(f\"Initialization complete. Map has {len(all_points_3d)} points. Initial baseline is {initial_baseline:.4f}\")\n",
    "\n",
    "\n",
    "MIN_TRANSLATION_THRESHOLD = initial_baseline * 0.40\n",
    "\n",
    "# --- 2. Main Sequential Loop ---\n",
    "all_camera_poses = [kf['pose'] for kf in keyframes]\n",
    "\n",
    "for i in range(2, len(image_files)):\n",
    "    print(f\"\\n--- Processing image {i+1}/{len(image_files)}: {image_files[i]} ---\")\n",
    "    \n",
    "    img_new_path = image_files[i]\n",
    "    kp_new, desc_new, _ = extract_features(img_new_path, 'sift')\n",
    "    \n",
    "    # Always match the new frame against the entire global map\n",
    "    matches = match_features(desc_new, map_point_descriptors, 'flann')\n",
    "    \n",
    "    # We get direct 2D-to-3D correspondences from this match\n",
    "    points_3d_for_pnp = np.array([all_points_3d[m.trainIdx] for m in matches])\n",
    "    points_2d_for_pnp = np.array([kp_new[m.queryIdx].pt for m in matches])\n",
    "\n",
    "    # Store which 3D point corresponds to which new keypoint for the map update\n",
    "    map_indices = np.array([m.trainIdx for m in matches])\n",
    "    kp_indices = np.array([m.queryIdx for m in matches])\n",
    "\n",
    "    MIN_PNP_MATCHES = 15\n",
    "    if len(matches) < MIN_PNP_MATCHES:\n",
    "        print(f\"Warning: Not enough map-to-frame matches ({len(matches)}). Skipping.\")\n",
    "        continue\n",
    "\n",
    "    retval, R_vec, t_new, inliers = cv2.solvePnPRansac(points_3d_for_pnp, points_2d_for_pnp, K, None, flags=cv2.SOLVEPNP_EPNP)\n",
    "    \n",
    "    if not retval or len(inliers) < MIN_PNP_MATCHES:\n",
    "        print(f\"Warning: solvePnPRansac failed or has too few inliers. Skipping.\")\n",
    "        continue\n",
    "        \n",
    "    R_new, _ = cv2.Rodrigues(R_vec)\n",
    "    current_pose = (R_new, t_new)\n",
    "    all_camera_poses.append(current_pose)\n",
    "    print(f\"Localization successful with {len(inliers)} inliers.\")\n",
    "    \n",
    "    # Decide if this frame should be a new keyframe to expand the map\n",
    "    last_keyframe_pose = keyframes[-1]['pose']\n",
    "    distance_moved = np.linalg.norm(t_new - last_keyframe_pose[1])\n",
    "    \n",
    "    if distance_moved > MIN_TRANSLATION_THRESHOLD:\n",
    "        print(f\"Distance ({distance_moved:.2f}) exceeds threshold ({MIN_TRANSLATION_THRESHOLD:.2f}). CREATING NEW KEYFRAME.\")\n",
    "        \n",
    "        new_keyframe = {'id': i, 'pose': current_pose, 'keypoints': kp_new, 'descriptors': desc_new}\n",
    "        keyframes.append(new_keyframe)\n",
    "        \n",
    "        # Match against the last keyframe to find brand-new points\n",
    "        last_keyframe_kp = keyframes[-1]['keypoints']\n",
    "        last_keyframe_desc = keyframes[-1]['descriptors']\n",
    "        new_kf_matches = match_features(desc_new, last_keyframe_desc, 'flann')\n",
    "        \n",
    "        # We need to find which of these matches are TRULY new, and not already in our map\n",
    "        # For simplicity, we triangulate all and add them, but a real system would filter duplicates\n",
    "        R_kf_new, t_kf_new = new_keyframe['pose']\n",
    "        R_kf_prev, t_kf_prev = last_keyframe_pose\n",
    "        new_3d_points = triangulate_points(kp_new, last_keyframe_kp, new_kf_matches, K, R_kf_new, t_kf_new, R_kf_prev, t_kf_prev)\n",
    "        \n",
    "        reasonable_mask = (new_3d_points[:, 2] > 0) & (new_3d_points[:, 2] < 50)\n",
    "        \n",
    "        new_points_count = 0\n",
    "        for j, match in enumerate(new_kf_matches):\n",
    "            if reasonable_mask[j]:\n",
    "                # Add the new 3D point to our global map\n",
    "                all_points_3d = np.vstack((all_points_3d, [new_3d_points[j]]))\n",
    "                # Add its descriptor to the global map descriptors\n",
    "                map_point_descriptors = np.vstack((map_point_descriptors, desc_new[match.queryIdx]))\n",
    "                new_points_count += 1\n",
    "\n",
    "        print(f\"Expanded map with {new_points_count} new points.\")\n",
    "        keyframes.append(new_keyframe)\n",
    "    else:\n",
    "        print(f\"Distance ({distance_moved:.2f}) is below threshold. Not creating a keyframe.\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_camera_frustum(ax, R, t, K, color, scale=0.5):\n",
    "    \"\"\"\n",
    "    Draws a camera frustum in a 3D plot.\n",
    "    \n",
    "    Args:\n",
    "        ax (matplotlib.axes._subplots.Axes3DSubplot): The 3D axes to draw on.\n",
    "        R (np.ndarray): The 3x3 rotation matrix of the camera.\n",
    "        t (np.ndarray): The 3x1 translation vector of the camera.\n",
    "        K (np.ndarray): The 3x3 intrinsic matrix.\n",
    "        color (str): The color for the frustum lines.\n",
    "        scale (float): The scale of the frustum to make it visible.\n",
    "    \"\"\"\n",
    "    # Get image dimensions from K (principal point is center)\n",
    "    img_height = K[1, 2] * 2\n",
    "    img_width = K[0, 2] * 2\n",
    "    \n",
    "    # Define the 4 corners of the image plane in pixel coordinates\n",
    "    # The points are homogenous, with a z-coordinate of 1\n",
    "    image_corners_pixels = np.array([\n",
    "        [0, 0, 1],\n",
    "        [img_width, 0, 1],\n",
    "        [img_width, img_height, 1],\n",
    "        [0, img_height, 1],\n",
    "        [0, 0, 1]  # To close the rectangle\n",
    "    ])\n",
    "    \n",
    "    # Invert the intrinsic matrix to back-project pixel coordinates to camera space\n",
    "    inv_K = np.linalg.inv(K)\n",
    "    \n",
    "    # Back-project the corners to the camera's coordinate system (on the z=1 plane)\n",
    "    frustum_corners_cam = inv_K @ image_corners_pixels.T\n",
    "    \n",
    "    # Scale the frustum for visualization\n",
    "    frustum_scaled = frustum_corners_cam * scale\n",
    "    \n",
    "    # The camera center is at the origin of the camera's coordinate system\n",
    "    # To get its world position, we use the inverse transformation: C = -R.T @ t\n",
    "    camera_center_world = -R.T @ t\n",
    "    \n",
    "    # Transform the frustum corners from camera coordinates to world coordinates\n",
    "    # P_world = R.T * P_cam + C\n",
    "    frustum_world = R.T @ frustum_scaled + camera_center_world\n",
    "    \n",
    "    # Draw the base of the frustum\n",
    "    ax.plot(frustum_world[0, :], frustum_world[1, :], frustum_world[2, :], color=color)\n",
    "    \n",
    "    # Draw the lines from the camera center to the frustum corners\n",
    "    for i in range(4):\n",
    "        ax.plot(\n",
    "            [camera_center_world[0, 0], frustum_world[0, i]],\n",
    "            [camera_center_world[1, 0], frustum_world[1, i]],\n",
    "            [camera_center_world[2, 0], frustum_world[2, i]],\n",
    "            color=color\n",
    "        )\n",
    "\n",
    "def visualize_full_reconstruction(points_3d, camera_poses, K):\n",
    "    \"\"\"\n",
    "    Creates a 3D visualization of the entire reconstructed scene.\n",
    "    \n",
    "    Args:\n",
    "        points_3d (np.ndarray): Nx3 array of 3D point coordinates.\n",
    "        camera_poses (list): A list of (R, t) tuples for each camera.\n",
    "        K (np.ndarray): The camera intrinsic matrix.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot the 3D point cloud\n",
    "    ax.scatter(points_3d[:, 0], points_3d[:, 1], points_3d[:, 2], c='blue', marker='.', s=2, alpha=0.5, label='Reconstructed 3D Points')\n",
    "    \n",
    "    # Plot each camera frustum\n",
    "    for i, (R, t) in enumerate(camera_poses):\n",
    "        # Use a colormap to give each camera a unique color\n",
    "        color = plt.cm.jet(i / len(camera_poses))\n",
    "        plot_camera_frustum(ax, R, t, K, color=color, scale=0.5)\n",
    "\n",
    "    ax.set_title('Full Sequential Structure from Motion Reconstruction')\n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Y axis')\n",
    "    ax.set_zlabel('Z axis')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Set the aspect ratio of the plot to be equal\n",
    "    # This is crucial for a correct interpretation of the 3D scene\n",
    "    max_range = np.array([\n",
    "        points_3d[:, 0].max() - points_3d[:, 0].min(),\n",
    "        points_3d[:, 1].max() - points_3d[:, 1].min(),\n",
    "        points_3d[:, 2].max() - points_3d[:, 2].min()\n",
    "    ]).max() / 2.0\n",
    "\n",
    "    mid_x = (points_3d[:, 0].max() + points_3d[:, 0].min()) * 0.5\n",
    "    mid_y = (points_3d[:, 1].max() + points_3d[:, 1].min()) * 0.5\n",
    "    mid_z = (points_3d[:, 2].max() + points_3d[:, 2].min()) * 0.5\n",
    "    \n",
    "    ax.set_xlim(mid_x - max_range, mid_x + max_range)\n",
    "    ax.set_ylim(mid_y - max_range, mid_y + max_range)\n",
    "    ax.set_zlim(mid_z - max_range, mid_z + max_range)\n",
    "    \n",
    "    # Set a nice viewing angle\n",
    "    ax.view_init(elev=-70, azim=-90)\n",
    "    \n",
    "    plt.show()\n",
    "print(\"\\nStep 3: Generating final 3D reconstruction...\")\n",
    "# (The visualization function 'visualize_full_reconstruction' and its helper\n",
    "# 'plot_camera_frustum' from the previous responses are assumed to be defined here)\n",
    "visualize_full_reconstruction(all_points_3d, all_camera_poses, K)\n",
    "print(\"\\nPoints and Camera poses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b2ce48-e5cd-4f92-b52e-19b0dc7ceff8",
   "metadata": {},
   "source": [
    "# ok, now to drape a texture\n",
    "\n",
    "We take all those points, and create triangles from a point to its two closest points, until we have a mesh supported by the points. Then we drape the images on top of the mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa000f2c-595c-485c-abd3-8e6e5988fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def export_textured_mesh(\n",
    "    all_points_3d, \n",
    "    keyframes, \n",
    "    image_files, \n",
    "    K,\n",
    "    output_prefix=\"mesh_reconstruction\",\n",
    "    max_edge_length=2.0,\n",
    "    texture_resolution=1024\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a triangulated mesh with proper texture mapping from SfM data.\n",
    "    \n",
    "    Process:\n",
    "    1. Create mesh topology using Delaunay triangulation\n",
    "    2. Assign textures to faces based on viewing angles\n",
    "    3. Create texture atlas with face-based UV mapping\n",
    "    4. Export as proper textured OBJ/MTL\n",
    "    \"\"\"\n",
    "    print(f\"\\nCreating textured mesh: {output_prefix}\")\n",
    "    \n",
    "    obj_filename = f\"{output_prefix}.obj\"\n",
    "    mtl_filename = f\"{output_prefix}.mtl\"\n",
    "    \n",
    "    # Load images\n",
    "    images = load_keyframe_images(keyframes, image_files)\n",
    "    if not images:\n",
    "        print(\"ERROR: No valid images loaded!\")\n",
    "        return\n",
    "    \n",
    "    # Step 1: Create mesh topology\n",
    "    faces, filtered_points = create_mesh_topology(all_points_3d, max_edge_length)\n",
    "    if len(faces) == 0:\n",
    "        print(\"ERROR: No valid faces created!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Created mesh with {len(filtered_points)} vertices and {len(faces)} faces\")\n",
    "    \n",
    "    # Step 2: Assign best camera to each face\n",
    "    face_cameras = assign_cameras_to_faces(faces, filtered_points, keyframes, K)\n",
    "    \n",
    "    # Step 3: Create texture atlas and UV coordinates\n",
    "    texture_filename, uv_coords = create_face_based_texture_atlas(\n",
    "        faces, filtered_points, face_cameras, keyframes, images, K, \n",
    "        texture_resolution, output_prefix\n",
    "    )\n",
    "    \n",
    "    # Step 4: Export mesh\n",
    "    export_textured_mesh_obj(\n",
    "        filtered_points, faces, uv_coords, \n",
    "        obj_filename, mtl_filename, texture_filename\n",
    "    )\n",
    "    \n",
    "    print(f\"Exported textured mesh: {obj_filename}\")\n",
    "    return {\n",
    "        'obj_file': obj_filename,\n",
    "        'mtl_file': mtl_filename, \n",
    "        'texture_file': texture_filename,\n",
    "        'vertices': len(filtered_points),\n",
    "        'faces': len(faces)\n",
    "    }\n",
    "\n",
    "\n",
    "def load_keyframe_images(keyframes, image_files):\n",
    "    \"\"\"Load and validate keyframe images.\"\"\"\n",
    "    images = {}\n",
    "    for kf in keyframes:\n",
    "        img_path = image_files[kf['id']]\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            images[kf['id']] = img\n",
    "            print(f\"Loaded image {kf['id']}: {img.shape}\")\n",
    "        else:\n",
    "            print(f\"Warning: Could not load {img_path}\")\n",
    "    return images\n",
    "\n",
    "\n",
    "def create_mesh_topology(points_3d, max_edge_length):\n",
    "    \"\"\"\n",
    "    Create mesh faces using 2.5D Delaunay triangulation.\n",
    "    Projects points to dominant plane for triangulation.\n",
    "    \"\"\"\n",
    "    print(\"Creating mesh topology...\")\n",
    "    \n",
    "    # Filter out outlier points\n",
    "    distances = np.linalg.norm(points_3d, axis=1)\n",
    "    distance_threshold = np.percentile(distances, 95)\n",
    "    valid_mask = distances < distance_threshold\n",
    "    filtered_points = points_3d[valid_mask]\n",
    "    \n",
    "    if len(filtered_points) < 3:\n",
    "        return [], []\n",
    "    \n",
    "    # Find dominant plane using PCA\n",
    "    centered_points = filtered_points - np.mean(filtered_points, axis=0)\n",
    "    _, _, V = np.linalg.svd(centered_points)\n",
    "    \n",
    "    # Project points onto the plane defined by first two principal components\n",
    "    projected_2d = centered_points @ V[:2].T\n",
    "    \n",
    "    # Delaunay triangulation in 2D\n",
    "    try:\n",
    "        tri = Delaunay(projected_2d)\n",
    "        triangles = tri.simplices\n",
    "    except:\n",
    "        print(\"Delaunay triangulation failed, using grid-based approach\")\n",
    "        return create_grid_based_mesh(filtered_points, max_edge_length)\n",
    "    \n",
    "    # Filter triangles by edge length in 3D\n",
    "    valid_faces = []\n",
    "    for triangle in triangles:\n",
    "        p1, p2, p3 = filtered_points[triangle]\n",
    "        \n",
    "        edge1 = np.linalg.norm(p2 - p1)\n",
    "        edge2 = np.linalg.norm(p3 - p2)\n",
    "        edge3 = np.linalg.norm(p1 - p3)\n",
    "        \n",
    "        max_edge = max(edge1, edge2, edge3)\n",
    "        \n",
    "        # Also check triangle area to avoid degenerate triangles\n",
    "        area = 0.5 * np.linalg.norm(np.cross(p2 - p1, p3 - p1))\n",
    "        \n",
    "        if max_edge < max_edge_length and area > 0.01:\n",
    "            valid_faces.append(triangle)\n",
    "    \n",
    "    print(f\"Created {len(valid_faces)} valid faces from {len(triangles)} triangles\")\n",
    "    return np.array(valid_faces), filtered_points\n",
    "\n",
    "\n",
    "def create_grid_based_mesh(points_3d, max_edge_length):\n",
    "    \"\"\"\n",
    "    Alternative mesh creation using nearest neighbor connections.\n",
    "    More robust but creates more conservative connectivity.\n",
    "    \"\"\"\n",
    "    print(\"Using grid-based mesh creation...\")\n",
    "    \n",
    "    # Build nearest neighbor graph\n",
    "    distances = cdist(points_3d, points_3d)\n",
    "    faces = []\n",
    "    \n",
    "    for i in range(len(points_3d)):\n",
    "        # Find nearby points\n",
    "        nearby_indices = np.where((distances[i] > 0) & (distances[i] < max_edge_length))[0]\n",
    "        \n",
    "        if len(nearby_indices) >= 2:\n",
    "            # Create triangles with current point and pairs of nearby points\n",
    "            for j in range(len(nearby_indices) - 1):\n",
    "                for k in range(j + 1, len(nearby_indices)):\n",
    "                    idx1, idx2, idx3 = i, nearby_indices[j], nearby_indices[k]\n",
    "                    \n",
    "                    # Check if this forms a reasonable triangle\n",
    "                    p1, p2, p3 = points_3d[[idx1, idx2, idx3]]\n",
    "                    area = 0.5 * np.linalg.norm(np.cross(p2 - p1, p3 - p1))\n",
    "                    \n",
    "                    if area > 0.01:  # Avoid degenerate triangles\n",
    "                        faces.append([idx1, idx2, idx3])\n",
    "    \n",
    "    return np.array(faces), points_3d\n",
    "\n",
    "\n",
    "def assign_cameras_to_faces(faces, vertices, keyframes, K):\n",
    "    \"\"\"\n",
    "    Assign the best camera/keyframe to each face based on viewing angle.\n",
    "    \"\"\"\n",
    "    print(\"Assigning cameras to faces...\")\n",
    "    \n",
    "    face_cameras = {}\n",
    "    \n",
    "    for face_idx, face in enumerate(faces):\n",
    "        # Calculate face center and normal\n",
    "        face_vertices = vertices[face]\n",
    "        face_center = np.mean(face_vertices, axis=0)\n",
    "        \n",
    "        # Calculate face normal using cross product\n",
    "        v1 = face_vertices[1] - face_vertices[0]\n",
    "        v2 = face_vertices[2] - face_vertices[0]\n",
    "        face_normal = np.cross(v1, v2)\n",
    "        face_normal = face_normal / (np.linalg.norm(face_normal) + 1e-8)\n",
    "        \n",
    "        best_camera = None\n",
    "        best_score = -1\n",
    "        \n",
    "        for kf in keyframes:\n",
    "            R, t = kf['pose']\n",
    "            \n",
    "            # Transform face center to camera coordinates\n",
    "            face_center_cam = R @ face_center.reshape(3, 1) + t\n",
    "            \n",
    "            # Skip if behind camera\n",
    "            if face_center_cam[2] <= 0:\n",
    "                continue\n",
    "            \n",
    "            # Project to check if face center is visible\n",
    "            face_center_img = K @ face_center_cam\n",
    "            u = face_center_img[0] / face_center_img[2]\n",
    "            v = face_center_img[1] / face_center_img[2]\n",
    "            \n",
    "            # Assuming image dimensions from K matrix\n",
    "            img_w, img_h = K[0, 2] * 2, K[1, 2] * 2\n",
    "            \n",
    "            if 0 <= u <= img_w and 0 <= v <= img_h:\n",
    "                # Calculate viewing score\n",
    "                viewing_direction = face_center_cam.flatten() / np.linalg.norm(face_center_cam)\n",
    "                \n",
    "                # Transform face normal to camera coordinates\n",
    "                face_normal_cam = R @ face_normal\n",
    "                \n",
    "                # Score based on alignment with viewing direction and distance\n",
    "                alignment = np.abs(np.dot(viewing_direction, face_normal_cam))\n",
    "                distance = np.linalg.norm(face_center_cam)\n",
    "                score = alignment / (distance + 1e-6)\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_camera = kf['id']\n",
    "        \n",
    "        face_cameras[face_idx] = best_camera\n",
    "    \n",
    "    print(f\"Assigned cameras to {len([c for c in face_cameras.values() if c is not None])} faces\")\n",
    "    return face_cameras\n",
    "\n",
    "\n",
    "def create_face_based_texture_atlas(faces, vertices, face_cameras, keyframes, images, K, \n",
    "                                   texture_resolution, output_prefix):\n",
    "    \"\"\"\n",
    "    Create texture atlas by packing face textures from their assigned cameras.\n",
    "    \"\"\"\n",
    "    print(\"Creating texture atlas...\")\n",
    "    \n",
    "    # Group faces by camera\n",
    "    camera_faces = {}\n",
    "    for face_idx, camera_id in face_cameras.items():\n",
    "        if camera_id is not None:\n",
    "            if camera_id not in camera_faces:\n",
    "                camera_faces[camera_id] = []\n",
    "            camera_faces[camera_id].append(face_idx)\n",
    "    \n",
    "    print(f\"Grouped faces into {len(camera_faces)} camera groups\")\n",
    "    \n",
    "    # Simple atlas layout: arrange camera textures in grid\n",
    "    grid_size = int(np.ceil(np.sqrt(len(camera_faces))))\n",
    "    atlas_size = texture_resolution * grid_size\n",
    "    texture_atlas = np.zeros((atlas_size, atlas_size, 3), dtype=np.uint8)\n",
    "    \n",
    "    # UV coordinates for each vertex\n",
    "    uv_coords = np.zeros((len(vertices), 2))\n",
    "    \n",
    "    camera_positions = {}\n",
    "    pos_idx = 0\n",
    "    \n",
    "    for camera_id in camera_faces:\n",
    "        if camera_id not in images:\n",
    "            continue\n",
    "            \n",
    "        # Calculate atlas position for this camera\n",
    "        row = pos_idx // grid_size\n",
    "        col = pos_idx % grid_size\n",
    "        atlas_x = col * texture_resolution\n",
    "        atlas_y = row * texture_resolution\n",
    "        camera_positions[camera_id] = (atlas_x, atlas_y)\n",
    "        \n",
    "        # Find keyframe for camera parameters\n",
    "        kf = next((k for k in keyframes if k['id'] == camera_id), None)\n",
    "        if kf is None:\n",
    "            continue\n",
    "            \n",
    "        R, t = kf['pose']\n",
    "        img = images[camera_id]\n",
    "        img_h, img_w = img.shape[:2]\n",
    "        \n",
    "        # Resize image to fit atlas tile\n",
    "        img_resized = cv2.resize(img, (texture_resolution, texture_resolution))\n",
    "        texture_atlas[atlas_y:atlas_y+texture_resolution, \n",
    "                     atlas_x:atlas_x+texture_resolution] = img_resized\n",
    "        \n",
    "        # Calculate UV coordinates for faces using this camera\n",
    "        for face_idx in camera_faces[camera_id]:\n",
    "            face = faces[face_idx]\n",
    "            \n",
    "            for vertex_idx in face:\n",
    "                vertex_3d = vertices[vertex_idx]\n",
    "                \n",
    "                # Project vertex to camera image\n",
    "                vertex_cam = R @ vertex_3d.reshape(3, 1) + t\n",
    "                if vertex_cam[2] > 0:\n",
    "                    vertex_img = K @ vertex_cam\n",
    "                    u_img = vertex_img[0] / vertex_img[2]\n",
    "                    v_img = vertex_img[1] / vertex_img[2]\n",
    "                    \n",
    "                    # Convert to normalized texture coordinates within this camera's atlas tile\n",
    "                    u_norm = float((u_img / img_w) * (texture_resolution / atlas_size) + (atlas_x / atlas_size))\n",
    "                    v_norm = float((v_img / img_h) * (texture_resolution / atlas_size) + (atlas_y / atlas_size))\n",
    "                    \n",
    "                    # Clamp to valid range\n",
    "                    u_norm = np.clip(u_norm, atlas_x/atlas_size, (atlas_x+texture_resolution)/atlas_size)\n",
    "                    v_norm = np.clip(v_norm, atlas_y/atlas_size, (atlas_y+texture_resolution)/atlas_size)\n",
    "                    \n",
    "                    uv_coords[vertex_idx, 0] = float(u_norm)\n",
    "                    uv_coords[vertex_idx, 1] = float(1.0 - v_norm)  # Flip V\n",
    "        \n",
    "        pos_idx += 1\n",
    "    \n",
    "    # Save texture atlas\n",
    "    texture_filename = f\"{output_prefix}_texture.jpg\"\n",
    "    cv2.imwrite(texture_filename, texture_atlas)\n",
    "    print(f\"Saved texture atlas ({atlas_size}x{atlas_size}) to {texture_filename}\")\n",
    "    \n",
    "    return texture_filename, uv_coords\n",
    "\n",
    "\n",
    "def export_textured_mesh_obj(vertices, faces, uv_coords, obj_filename, mtl_filename, texture_filename):\n",
    "    \"\"\"\n",
    "    Export the textured mesh as OBJ/MTL files.\n",
    "    \"\"\"\n",
    "    # Write MTL file\n",
    "    with open(mtl_filename, 'w') as f:\n",
    "        f.write(\"# Material file for textured mesh\\n\")\n",
    "        f.write(\"newmtl TexturedMesh\\n\")\n",
    "        f.write(\"Ka 1.0 1.0 1.0\\n\")\n",
    "        f.write(\"Kd 1.0 1.0 1.0\\n\")\n",
    "        f.write(\"Ks 0.0 0.0 0.0\\n\")\n",
    "        f.write(\"d 1.0\\n\")\n",
    "        f.write(\"illum 1\\n\")\n",
    "        f.write(f\"map_Kd {texture_filename}\\n\")\n",
    "    \n",
    "    # Write OBJ file\n",
    "    with open(obj_filename, 'w') as f:\n",
    "        f.write(\"# Textured mesh from Structure from Motion\\n\")\n",
    "        f.write(f\"mtllib {mtl_filename}\\n\\n\")\n",
    "        \n",
    "        # Write vertices\n",
    "        f.write(\"# Vertices\\n\")\n",
    "        for vertex in vertices:\n",
    "            f.write(f\"v {vertex[0]:.6f} {vertex[1]:.6f} {vertex[2]:.6f}\\n\")\n",
    "        \n",
    "        # Write texture coordinates\n",
    "        f.write(\"\\n# Texture coordinates\\n\")\n",
    "        for uv in uv_coords:\n",
    "            f.write(f\"vt {uv[0]:.6f} {uv[1]:.6f}\\n\")\n",
    "        \n",
    "        # Write faces with texture coordinates\n",
    "        f.write(\"\\nusemtl TexturedMesh\\n\")\n",
    "        f.write(\"# Faces\\n\")\n",
    "        for face in faces:\n",
    "            # OBJ uses 1-based indexing\n",
    "            v1, v2, v3 = face + 1\n",
    "            f.write(f\"f {v1}/{v1} {v2}/{v2} {v3}/{v3}\\n\")\n",
    "    \n",
    "    print(f\"Exported textured mesh to {obj_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4569305a-98c2-4017-970a-6148896e89a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = export_textured_mesh(\n",
    "     all_points_3d,\n",
    "     keyframes,\n",
    "     image_files,\n",
    "     K,\n",
    "     \"castle_mesh\",\n",
    "     max_edge_length=2.0,\n",
    "     texture_resolution=512\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd580ede-457f-46d3-8bec-a736796ed2ca",
   "metadata": {},
   "source": [
    "So that's more or less how it all works. You take the .obj, .mtl, and the texture.jpg files and zip those up. Then, you can upload them to a 3d object viewer like sketchfab.com. Or go to [https://3dviewer.net/](https://3dviewer.net/) and drag the .obj, .mtl, and the texture jpg onto the screen.\n",
    "\n",
    "Our castle looks a bit ... trippy. And that's because I haven't bothered to figure out yet all the niggling details that would resolve the outliers and odd bits of data artefacts. Because the approach here is meant to show you the basic logic and flow, the final output isn't as nice as a more robust dataset and approach might create.\n",
    "\n",
    "Compare the result with what you might get with an older piece of software called [Regard3d](https://www.regard3d.org/index.php/download). It bundles all of the steps we've done here together into a user interface and critically, shows you the output of each step and allows you to play with all the possible variables so that you can address the issues visible in our result here. Windows users might try [Zephyr Free](https://www.3dflow.net/3df-zephyr-free/); there are plenty of apps for IOS and Android, if you want to try things that way.\n",
    "\n",
    "A more technically sophisticated technique is 'gaussian splatting', which takes a video as input. The technique uses a kind of neural network approach for rendering the data. Essentially, while the sfm approach we used here identifies points in the images, gaussian splatting considers each point as a whole distribution of values. You can imagine what the math would be like. The upshot is you get much better results that aren't as fragile as the sfm approach. You can give it a try [with this google colab notebook](https://colab.research.google.com/github/benyoon1/gaussian-splat-colab/blob/main/gsplat_colab_template.ipynb#scrollTo=Jg-_TYBaJkcD). Here's some [introductory material explaining the main idea](https://huggingface.co/blog/gaussian-splatting). And as it happens, [poly.cam](https://poly.cam) can do both approaches so you can see the difference for yourself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7e34ba-b1a3-477e-966e-1c777eb11457",
   "metadata": {},
   "source": [
    "# Your turn\n",
    "Make a folder in the week 10 workbench. Put your photos from the graveyard in there. Make a copy of these notebook, then open it. Go back to the section 1 of the notebook and change the data input to point to your folder of images (so not the download cells, right!). Work through the notebook again, but swapping in your data at each step. See how it goes.\n",
    "\n",
    "Then, go try [poly.cam](https://poly.cam/). You can get a free account and use their platform to turn both images and videos into photogrammetric sfm models _or_ guassian splats - but be careful to not get upsold beyond the free tier. These folks have professional developers working on making their algorithms the best in the business, so you'll definitely get better results than my code, as long as your input images have sufficient overlap. Use your images and videos you captured when you were at the graveyard. Download a model it makes and put it in your github repository. Evaluate the quality of your model. What might your model be *good for*?\n",
    "\n",
    "Also - explore the poly.cam interface carefully. If a model is built right, you can actually read measurements and dimensions from the 3d model, and that is certainly handy for communicating the results of your research or making it useful for others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997cd3d-a6b3-461b-b4c1-04f47c585cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
