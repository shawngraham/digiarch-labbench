{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c987f3ce-87ca-4daf-acef-81bfc7849f2b",
   "metadata": {},
   "source": [
    "There are any number of software packages that will take your photos and work out how the images overlap to combine into a 3d model. Windows users might want to look up 3DF Zephyr Free; Mac can try Meshroom (but it's not for the faint of heart [link](https://alicevision.org/#meshroom))\n",
    "\n",
    "In this notebook, we have gently modified the demonstration process of Florent Poux, who wrote: [https://towardsdatascience.com/master-the-3d-reconstruction-process-step-by-step-guide/](https://towardsdatascience.com/master-the-3d-reconstruction-process-step-by-step-guide/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74525c74-6432-410c-96e4-e47754dac03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's grab a fun set of test images\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Create the images directory\n",
    "os.makedirs('images', exist_ok=True)\n",
    "\n",
    "# Base URL pattern\n",
    "base_url = 'https://github.com/snavely/bundler_sfm/blob/master/examples/kermit/kermit{:03d}.jpg?raw=true'\n",
    "\n",
    "# Loop through kermit000.jpg to kermit010.jpg\n",
    "for i in range(11):  # 0 to 10 inclusive\n",
    "    url = base_url.format(i)\n",
    "    filename = f'images/kermit{i:03d}.jpg'\n",
    "    \n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download kermit{i:03d}.jpg: {e}\")\n",
    "\n",
    "print(\"Download process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b797d-a7e9-4a6f-a482-b7e83c0d19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% SECTION 1: Natural Feature Extraction\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_features(image_path, feature_method='sift', max_features=2000):\n",
    "    \"\"\"\n",
    "    Extract features from an image using different methods.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the image in color and convert to grayscale\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read image at {image_path}\")\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Initialize feature detector based on method\n",
    "    if feature_method.lower() == 'sift':\n",
    "        detector = cv2.SIFT_create(nfeatures=max_features)\n",
    "    elif feature_method.lower() == 'surf':\n",
    "        # Note: SURF is patented and may not be available in all OpenCV distributions\n",
    "        detector = cv2.xfeatures2d.SURF_create(400)  # Adjust threshold as needed\n",
    "    elif feature_method.lower() == 'orb':\n",
    "        detector = cv2.ORB_create(nfeatures=max_features)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported feature method: {feature_method}\")\n",
    "    \n",
    "    # Detect and compute keypoints and descriptors\n",
    "    keypoints, descriptors = detector.detectAndCompute(gray, None)\n",
    "    \n",
    "    # Create visualization\n",
    "    img_with_features = cv2.drawKeypoints(\n",
    "        img, keypoints, None, \n",
    "        flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    "    )\n",
    "    \n",
    "    print(f\"Extracted {len(keypoints)} {feature_method.upper()} features\")\n",
    "    \n",
    "    return keypoints, descriptors, img_with_features\n",
    "\n",
    "image_path = \"images/kermit001.jpg\"  \n",
    "\n",
    "# Extract features with different methods\n",
    "kp_sift, desc_sift, vis_sift = extract_features(image_path, 'sift')\n",
    "kp_orb, desc_orb, vis_orb = extract_features(image_path, 'orb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf78f97-7ebf-4ca3-9b07-713b7935572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "plt.figure(figsize=(12, 6))\n",
    "    \n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(f'SIFT Features ({len(kp_sift)})')\n",
    "plt.imshow(cv2.cvtColor(vis_sift, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(f'ORB Features ({len(kp_orb)})')\n",
    "plt.imshow(cv2.cvtColor(vis_orb, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca213a94-905e-4aee-8a49-0167a27627c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% SECTION 2: Feature Matching\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def match_features(descriptors1, descriptors2, method='flann', ratio_thresh=0.75):\n",
    "    \"\"\"\n",
    "    Match features between two images using different methods.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert descriptors to appropriate type if needed\n",
    "    if descriptors1 is None or descriptors2 is None:\n",
    "        return []\n",
    "    \n",
    "    if method.lower() == 'flann':\n",
    "        # FLANN parameters\n",
    "        if descriptors1.dtype != np.float32:\n",
    "            descriptors1 = np.float32(descriptors1)\n",
    "        if descriptors2.dtype != np.float32:\n",
    "            descriptors2 = np.float32(descriptors2)\n",
    "            \n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "        search_params = dict(checks=50)  # Higher values = more accurate but slower\n",
    "        \n",
    "        flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n",
    "    else:  # Brute Force\n",
    "        # For ORB descriptors\n",
    "        if descriptors1.dtype == np.uint8:\n",
    "            bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "        else:  # For SIFT and SURF descriptors\n",
    "            bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "        \n",
    "        matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "    \n",
    "    # Apply Lowe's ratio test\n",
    "    good_matches = []\n",
    "    for match in matches:\n",
    "        if len(match) == 2:  # Sometimes fewer than 2 matches are returned\n",
    "            m, n = match\n",
    "            if m.distance < ratio_thresh * n.distance:\n",
    "                good_matches.append(m)\n",
    "    \n",
    "    return good_matches\n",
    "\n",
    "def visualize_matches(img1, kp1, img2, kp2, matches, max_display=100):\n",
    "    \"\"\"\n",
    "    Create a visualization of feature matches between two images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Limit the number of matches to display\n",
    "    matches_to_draw = matches[:min(max_display, len(matches))]\n",
    "    \n",
    "    # Create match visualization\n",
    "    match_img = cv2.drawMatches(\n",
    "        img1, kp1, img2, kp2, matches_to_draw, None,\n",
    "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "    )\n",
    "    \n",
    "    return match_img\n",
    "\n",
    "# Load two images\n",
    "img1_path = \"images/kermit000.jpg\"  \n",
    "img2_path = \"images/kermit001.jpg\"\n",
    "    \n",
    "# Extract features using SIFT (or your preferred method)\n",
    "kp1, desc1, _ = extract_features(img1_path, 'sift')\n",
    "kp2, desc2, _ = extract_features(img2_path, 'sift')\n",
    "    \n",
    "# Match features\n",
    "good_matches = match_features(desc1, desc2, method='flann')\n",
    "    \n",
    "print(f\"Found {len(good_matches)} good matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd251c-0bf7-4490-8c0f-e94fdd5dd6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize matches\n",
    "img1 = cv2.imread(img1_path)\n",
    "img2 = cv2.imread(img2_path)\n",
    "match_visualization = visualize_matches(img1, kp1, img2, kp2, good_matches)\n",
    "    \n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cv2.cvtColor(match_visualization, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f\"Feature Matches: {len(good_matches)}\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df5aeb-728d-4675-a5f4-7e8cd888064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% SECTION 3: Structure from Motion\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def estimate_pose(kp1, kp2, matches, K, method=cv2.RANSAC, prob=0.999, threshold=1.0):\n",
    "    \"\"\"\n",
    "    Estimate the relative pose between two cameras using matched features.\n",
    "    \"\"\"\n",
    "    # Extract matched points\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches])\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches])\n",
    "    \n",
    "    # Estimate essential matrix\n",
    "    E, mask = cv2.findEssentialMat(pts1, pts2, K, method, prob, threshold)\n",
    "    \n",
    "    # Recover pose from essential matrix\n",
    "    _, R, t, mask = cv2.recoverPose(E, pts1, pts2, K, mask=mask)\n",
    "    \n",
    "    inlier_matches = [matches[i] for i in range(len(matches)) if mask[i] > 0]\n",
    "    print(f\"Estimated pose with {np.sum(mask)} inliers out of {len(matches)} matches\")\n",
    "    \n",
    "    return R, t, mask, inlier_matches\n",
    "\n",
    "def triangulate_points(kp1, kp2, matches, K, R1, t1, R2, t2):\n",
    "    \"\"\"\n",
    "    Triangulate 3D points from two views.\n",
    "    \"\"\"\n",
    "    # Extract matched points\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches])\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches])\n",
    "    \n",
    "    # Create projection matrices\n",
    "    P1 = np.dot(K, np.hstack((R1, t1)))\n",
    "    P2 = np.dot(K, np.hstack((R2, t2)))\n",
    "    \n",
    "    # Triangulate points\n",
    "    points_4d = cv2.triangulatePoints(P1, P2, pts1.T, pts2.T)\n",
    "    \n",
    "    # Convert to 3D points\n",
    "    points_3d = points_4d[:3] / points_4d[3]\n",
    "    \n",
    "    return points_3d.T\n",
    "\n",
    "def visualize_points_and_cameras(points_3d, R1, t1, R2, t2):\n",
    "    \"\"\"\n",
    "    Visualize 3D points and camera positions.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot points\n",
    "    ax.scatter(points_3d[:, 0], points_3d[:, 1], points_3d[:, 2], c='b', s=1)\n",
    "    \n",
    "    # Helper function to create camera visualization\n",
    "    def plot_camera(R, t, color):\n",
    "        # Camera center\n",
    "        center = -R.T @ t\n",
    "        ax.scatter(center[0], center[1], center[2], c=color, s=100, marker='o')\n",
    "        \n",
    "        # Camera axes (showing orientation)\n",
    "        axes_length = 0.5  # Scale to make it visible\n",
    "        for i, c in zip(range(3), ['r', 'g', 'b']):\n",
    "            axis = R.T[:, i] * axes_length\n",
    "            ax.quiver(center[0], center[1], center[2], \n",
    "                      axis[0], axis[1], axis[2], \n",
    "                      color=c, arrow_length_ratio=0.1)\n",
    "    \n",
    "    # Plot cameras\n",
    "    plot_camera(R1, t1, 'red')\n",
    "    plot_camera(R2, t2, 'green')\n",
    "    \n",
    "    ax.set_title('3D Reconstruction: Points and Cameras')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    \n",
    "    # Try to make axes equal\n",
    "    max_range = np.max([\n",
    "        np.max(points_3d[:, 0]) - np.min(points_3d[:, 0]),\n",
    "        np.max(points_3d[:, 1]) - np.min(points_3d[:, 1]),\n",
    "        np.max(points_3d[:, 2]) - np.min(points_3d[:, 2])\n",
    "    ])\n",
    "    \n",
    "    mid_x = (np.max(points_3d[:, 0]) + np.min(points_3d[:, 0])) * 0.5\n",
    "    mid_y = (np.max(points_3d[:, 1]) + np.min(points_3d[:, 1])) * 0.5\n",
    "    mid_z = (np.max(points_3d[:, 2]) + np.min(points_3d[:, 2])) * 0.5\n",
    "    \n",
    "    ax.set_xlim(mid_x - max_range * 0.5, mid_x + max_range * 0.5)\n",
    "    ax.set_ylim(mid_y - max_range * 0.5, mid_y + max_range * 0.5)\n",
    "    ax.set_zlim(mid_z - max_range * 0.5, mid_z + max_range * 0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example camera intrinsic matrix (replace with your calibrated values)\n",
    "K = np.array([\n",
    "    [1000, 0, 320],\n",
    "    [0, 1000, 240],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "# For first camera, we use identity rotation and zero translation\n",
    "R1 = np.eye(3)\n",
    "t1 = np.zeros((3, 1))\n",
    "\n",
    "# Load images, extract features, and match as in previous sections\n",
    "img1_path = \"images/kermit000.jpg\"  # \n",
    "img2_path = \"images/kermit000.jpg\"\n",
    "\n",
    "img1 = cv2.imread(img1_path)\n",
    "img2 = cv2.imread(img2_path)\n",
    "\n",
    "\n",
    "kp1, desc1, _ = extract_features(img1_path, 'sift')\n",
    "kp2, desc2, _ = extract_features(img2_path, 'sift')\n",
    "\n",
    "matches = match_features(desc1, desc2, method='flann')\n",
    "\n",
    "# Estimate pose of second camera relative to first\n",
    "R2, t2, mask, inliers = estimate_pose(kp1, kp2, matches, K)\n",
    "\n",
    "# Triangulate points\n",
    "points_3d = triangulate_points(kp1, kp2, inliers, K, R1, t1, R2, t2)\n",
    "\n",
    "# Visualize results\n",
    "visualize_points_and_cameras(points_3d, R1, t1, R2, t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd580ede-457f-46d3-8bec-a736796ed2ca",
   "metadata": {},
   "source": [
    "This notebook will not give you a completed 3d model; there are a few more steps left in terms of making a complete pipeline that iterates over every photo, and then creates a texture for the point cloud. \n",
    "\n",
    "This is an older piece of software [Regard3d](https://www.regard3d.org/index.php/download) that you might want to try out; it bundles all of those steps together into a user interface. Windows users might try [Zephyr Free](https://www.3dflow.net/3df-zephyr-free/); there are plenty of apps for IOS and Android, if you want to try things that way.\n",
    "\n",
    "A more technically sophisticated technique is 'gaussian splatting', which takes a video as input. The technique uses a kind of neural network approach for rendering the data. You can give it a try [with this google colab notebook](https://colab.research.google.com/github/benyoon1/gaussian-splat-colab/blob/main/gsplat_colab_template.ipynb#scrollTo=Jg-_TYBaJkcD) [and here's some introductory material explaining the main idea](https://huggingface.co/blog/gaussian-splatting).\n",
    "\n",
    "Another place you might try is [poly.cam](https://poly.cam/). You can get a free account and use their platform to turn both images and videos into photogrammetric sfm models _or_ guassian splats - but be careful to not get upsold beyond the free tier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
