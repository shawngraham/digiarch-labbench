{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34cb1362-6690-4aea-acee-80af85b593c4",
   "metadata": {},
   "source": [
    "# Building an image classifier\n",
    "\n",
    "We're going to build an archaeological image classifier using Tensorflow, a library of python packages designed to perform machine learning tasks. We are going to train a model to recognize different kinds of Roman pottery.\n",
    "\n",
    "The data we're going to use comes from [Potsherd: Atlas of Roman Pottery](http://potsherd.net/atlas/potsherd). I've already downloaded the dataset for you, and there's a block of code below designed to retrieve it for you. It's not the full database; rather, I just put together a small set of ten or so different types of pottery, just to keep things manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63797623-551a-4eef-97f3-adc704657560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Retrieving the resource located at the URL and storing it in a zip file\n",
    "url = \"https://github.com/shawngraham/hist3000/blob/master/static/data/data.zip?raw=true\"\n",
    "urllib.request.urlretrieve(url, \"data.zip\")\n",
    "\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75799628-44cd-49ce-8c50-2b97fc98f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the tensorflow package\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7687d144-3aaf-4054-9aab-39a49c95b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up matplotlib for better plots\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e99f861-08cd-4541-b3c5-3bc91e27cacb",
   "metadata": {},
   "source": [
    "# Load the Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a45c83-35e5-4cdb-8177-b6dc7dd8f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data/ware\")\n",
    "\n",
    "# List all classes (subdirectories)\n",
    "class_names = [d.name for d in data_dir.iterdir() if d.is_dir()]\n",
    "print(f\"üìÅ Found {len(class_names)} classes: {class_names}\")\n",
    "    \n",
    "# Count images in each class\n",
    "for class_name in class_names:\n",
    "    class_path = data_dir / class_name\n",
    "    image_count = len(list(class_path.glob(\"*\")))\n",
    "    print(f\"   {class_name}: {image_count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6909f5cf-25f8-4557-9c31-b1409a4062f5",
   "metadata": {},
   "source": [
    "# Split the data into training and testing datasets\n",
    "\n",
    "We typically split data so that 80% of it is used for training, and 20% is held back for testing and validation.\n",
    "\n",
    "First call (subset=\"training\"):\n",
    "\n",
    "+ validation_split=0.2 ‚Üí \"Split the data: 80% training, 20% validation\"\n",
    "+ subset=\"training\" ‚Üí \"Give me the 80% training portion\"\n",
    "+ Result: train_ds gets 80% of the data\n",
    "\n",
    "\n",
    "Second call (subset=\"validation\"):\n",
    "\n",
    "+ validation_split=0.2 ‚Üí \"Split the data: 80% training, 20% validation\" (same split!)\n",
    "+ subset=\"validation\" ‚Üí \"Give me the 20% validation portion\"\n",
    "+ Result: val_ds gets 20% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da1205f7-cb5d-4586-9504-de5af6ee87e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56 files belonging to 12 classes.\n",
      "Using 12 files for training.\n",
      "Found 56 files belonging to 12 classes.\n",
      "Using 11 files for validation.\n",
      "üìä Number of classes: 12\n",
      "üìä Class names: ['AHGW', 'AOMO', 'ARGO', 'B4', 'BB1', 'BB2', 'C189', 'CGBL', 'CGCC', 'CGGL', 'CGMW', 'CGTS']\n"
     ]
    }
   ],
   "source": [
    "# Image parameters\n",
    "IMG_HEIGHT = 224  # Standard size for most pre-trained models\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create training dataset (80% of data)\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,  \n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'  # One-hot encoding for multiple classes\n",
    ")\n",
    "\n",
    "# Create validation dataset (20% of data)\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "# Get class names from the dataset\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print(f\"üìä Number of classes: {num_classes}\")\n",
    "print(f\"üìä Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2499f4d5-f223-4ddb-8c04-7ca4f1d49810",
   "metadata": {},
   "source": [
    "# Visualize Some Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde66aa7-5cf1-4e58-953b-4ee254a16aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some of our training images\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Take one batch of images and labels\n",
    "for images, labels in train_ds.take(1):\n",
    "    # Show first 9 images\n",
    "    for i in range(9):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        \n",
    "        # Convert image to displayable format\n",
    "        img = images[i].numpy().astype(\"uint8\")\n",
    "        plt.imshow(img)\n",
    "        \n",
    "        # Get the class name from the one-hot encoded label\n",
    "        class_idx = np.argmax(labels[i])\n",
    "        plt.title(f\"Class: {class_names[class_idx]}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Sample Training Images\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521613e-ec8a-4691-af3d-7549b710a8ce",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa080a35-02a8-45c4-9caa-66e0a805218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to [0, 1] range\n",
    "# This helps the neural network train more effectively\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# Apply normalization to both datasets\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# Configure datasets for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(\"‚úÖ Data preprocessing complete!\")\n",
    "print(\"üìã Images are now normalized to [0, 1] range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee582bc-098e-4ad1-91de-9829afc5cc77",
   "metadata": {},
   "source": [
    "# Build a neural network model\n",
    "On the basis of a pre-trained one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b25af-37b4-4092-ac10-fca0601e8157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use transfer learning with MobileNetV2\n",
    "# This is a pre-trained model that already knows about images\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "    include_top=False,  # Don't include the final classification layer\n",
    "    weights='imagenet'  # Use weights trained on ImageNet\n",
    ")\n",
    "\n",
    "# Freeze the base model (don't train it)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build our custom model\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,                                    # Pre-trained feature extractor\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),     # Convert features to 1D\n",
    "    tf.keras.layers.Dropout(0.2),                 # Prevent overfitting\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')  # Final classification layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(\"üß† Neural Network Architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5792f80c-ed7a-4012-b15f-66c51830e206",
   "metadata": {},
   "source": [
    "# Now we train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea25fe0-c156-4483-af44-7bfd56d6be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training parameters\n",
    "EPOCHS = 10  # Number of times to go through the entire dataset; more generally but not always gives better results\n",
    "\n",
    "print(f\"üöÄ Starting training for {EPOCHS} epochs...\")\n",
    "print(\"This might take a few minutes...\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1  # Show progress\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b58da78-bd73-417c-8e95-f15a99233f44",
   "metadata": {},
   "source": [
    "# Now we assess how well the model has been trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c918d-8102-4ec6-b537-fb2645369083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "ax1.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "ax1.set_title('Model Accuracy Over Time')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot training & validation loss\n",
    "ax2.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "ax2.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "ax2.set_title('Model Loss Over Time')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final results\n",
    "final_acc = history.history['val_accuracy'][-1]\n",
    "print(f\"üéØ Final Validation Accuracy: {final_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302ca74-973d-4e4d-9bf2-3d829775546b",
   "metadata": {},
   "source": [
    "# Make a prediction: given this image, what kind of ware is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40900081-ed16-4fbc-a063-872747f575b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incidentally, you don't want to be retraining a model everytime you want to do some classification.\n",
    "# So we can save it like this:\n",
    "\n",
    "model_path = \"image_classifier_model.h5\"\n",
    "model.save(model_path)\n",
    "print(f\"üíæ Model saved as: {model_path}\")\n",
    "\n",
    "# And then, for the functions below, if you were resuming work, you could load the model with:\n",
    "# loaded_model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae38a975-5188-46b1-9aa2-b5106faf886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on a batch of images\n",
    "def predict_and_display(dataset, num_images=6):\n",
    "    \"\"\"Display images with their predicted and actual labels\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Get one batch of images\n",
    "    for images, labels in dataset.take(1):\n",
    "        # Make predictions\n",
    "        predictions = model.predict(images)\n",
    "        \n",
    "        # Display the first num_images\n",
    "        for i in range(min(num_images, len(images))):\n",
    "            plt.subplot(2, 3, i + 1)\n",
    "            \n",
    "            # Show image\n",
    "            img = images[i].numpy()\n",
    "            plt.imshow(img)\n",
    "            \n",
    "            # Get actual and predicted classes\n",
    "            actual_class = class_names[np.argmax(labels[i])]\n",
    "            predicted_class = class_names[np.argmax(predictions[i])]\n",
    "            confidence = np.max(predictions[i])\n",
    "            \n",
    "            # Set title color based on correctness\n",
    "            color = 'green' if actual_class == predicted_class else 'red'\n",
    "            \n",
    "            plt.title(f'Actual: {actual_class}\\n'\n",
    "                     f'Predicted: {predicted_class}\\n'\n",
    "                     f'Confidence: {confidence:.2%}', \n",
    "                     color=color)\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('Model Predictions on Validation Images', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show predictions\n",
    "predict_and_display(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f73cb-dc5c-456b-90a5-b58908141e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict a single image\n",
    "def predict_single_image(image_path):\n",
    "    \"\"\"Predict the class of a single image\"\"\"\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    img = tf.keras.preprocessing.image.load_img(\n",
    "        image_path, \n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH)\n",
    "    )\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Add batch dimension\n",
    "    img_array = img_array / 255.0  # Normalize\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = np.max(predictions[0])\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'Predicted: {predicted_class}\\nConfidence: {confidence:.2%}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show confidence for all classes\n",
    "    print(\"üîç Confidence scores for all classes:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"   {class_name}: {predictions[0][i]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756207b0-a6d0-4e06-be46-1615b68a42b0",
   "metadata": {},
   "source": [
    "# Go out and find a picture of something Roman, and predict!\n",
    "\n",
    "Go to the [Potshered: Atlas of Roman Pottery](https://potsherd.net/atlas/potsherd) and find some pottery that interests you. You can then right-click on an image and select 'copy url to image'. Then we can feed it to our model to see what the model predicts using this command:\n",
    "\n",
    "```\n",
    "predict_single_image(\"path/to/your/test/image.jpg\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a98fc66-1060-4120-8b8a-48046930a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from PIL import Image\n",
    "\n",
    "# Retrieving the resource located at the URL\n",
    "# and storing it in the file name a.png\n",
    "url = \"https://potsherd.net/atlas/gallery2/ITTS/ITTS-stamp.jpg\"\n",
    "urllib.request.urlretrieve(url, \"ITTS-stamp.jpg\")\n",
    "\n",
    "# Opening the image and displaying it (to confirm its presence)\n",
    "img = Image.open(r\"ITTS-stamp.jpg\")\n",
    "img.show()\n",
    "\n",
    "predict_single_image(\"ITTS-stamp.jpg\")\n",
    "\n",
    "# this example image won't be too good a result, since we didn't train the model on any Terra Sigillata! \n",
    "# But go look at the ware type in the atlas - maybe the failure of the model also teaches us something.\n",
    "# And maybe the ware that you select will be correctly identified!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821acc7b-313d-4061-9848-7ce943734037",
   "metadata": {},
   "source": [
    "## Go Further!\n",
    "\n",
    "Find more classes of archaeological materials and build your own classifier. The key element is finding *enough* images - more is better - and then organizing them where each folder name is the name of the category. \n",
    "\n",
    "Then, you re-run this notebook but point the initial path to your own data/categories/category1 etc folder, ie the line where it says ```data_dir = Path(\"data/ware\")``` you change."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
