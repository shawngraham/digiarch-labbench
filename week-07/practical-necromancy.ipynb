{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9a5787-c0db-48b8-b0e2-f21348fa051b",
   "metadata": {},
   "source": [
    "# Practical Necromancy\n",
    "\n",
    "We're going to resurrect [Flinders Petrie](https://en.wikipedia.org/wiki/Flinders_Petrie).\n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/5/5a/Flinders_Petrie.jpg)\n",
    "\n",
    "This demo is based on the colab notebook by Max Woolf (see his [2019 blog post](https://minimaxir.com/2019/09/howto-gpt2/)).\n",
    "\n",
    "We're using an older model because a) we don't always need the latest flashy thing b) the older model can be explored c) we should try understanding the foundational technologies before jumping to the latest flashy thing. A and C are really aspects of the same argument I suppose.\n",
    "\n",
    "I also want you to see what generative 'ai' looks like when we pull back the curtain and remove the chatbot and its illusion of intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff470d5-a5d1-4f60-aa81-710f49b8fb6c",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3808474-1381-4ee8-a50c-bd46c0187408",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gpt-2-simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51bff19d-4441-4ec4-8328-8504f34cce1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 3.26Git/s]                                                     \n",
      "Fetching encoder.json: 1.05Mit [00:00, 2.98Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 4.33Git/s]                                                   \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [01:05, 7.55Mit/s]                                  \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 1.51Git/s]                                               \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 4.28Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 4.75Mit/s]                                                      \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gpt_2_simple as gpt2\n",
    "from datetime import datetime\n",
    "gpt2.download_gpt2(model_name=\"124M\") \n",
    "\n",
    "#124 million parameters! State of the art, only a few short years ago. \n",
    "# Now, something that you can run in a notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913875aa-385c-41a2-9251-96524fe9c7df",
   "metadata": {},
   "source": [
    "Now we're going to make a *big* assumption. We're going to assume that the published works of Flinders Petrie contain something meaningful about the essence of the man. \n",
    "\n",
    "We're going to grab these from the Gutenberg Project (a repository of public domain literature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82433a63-3882-4371-b643-46ef4d64bcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pg7386.txt...\n",
      "Successfully downloaded pg7386.txt\n",
      "Downloading pg70049.txt...\n",
      "Successfully downloaded pg70049.txt\n",
      "Downloading pg52570.txt...\n",
      "Successfully downloaded pg52570.txt\n",
      "Downloading pg63311.txt...\n",
      "Successfully downloaded pg63311.txt\n",
      "Downloading pg56095.txt...\n",
      "Successfully downloaded pg56095.txt\n",
      "All downloads completed!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# List of URLs to download\n",
    "urls = [\n",
    "    \"https://www.gutenberg.org/cache/epub/7386/pg7386.txt\",\n",
    "    \"https://www.gutenberg.org/cache/epub/70049/pg70049.txt\",\n",
    "    \"https://www.gutenberg.org/cache/epub/52570/pg52570.txt\",\n",
    "    \"https://www.gutenberg.org/cache/epub/63311/pg63311.txt\",\n",
    "    \"https://www.gutenberg.org/cache/epub/56095/pg56095.txt\"\n",
    "]\n",
    "\n",
    "# Loop through each URL and download the file\n",
    "for url in urls:\n",
    "    # Extract the filename from the URL\n",
    "    filename = url.split('/')[-1]\n",
    "    \n",
    "    try:\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(f\"Successfully downloaded {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {filename}: {e}\")\n",
    "\n",
    "print(\"All downloads completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c474c186-89a9-47c3-9e95-23751068d82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding pg7386.txt to petrie.txt\n",
      "Adding pg70049.txt to petrie.txt\n",
      "Adding pg52570.txt to petrie.txt\n",
      "Adding pg63311.txt to petrie.txt\n",
      "Adding pg56095.txt to petrie.txt\n",
      "Files concatenated into petrie.txt\n"
     ]
    }
   ],
   "source": [
    "# join those files together\n",
    "def concatenate_files(output_filename=\"petrie.txt\"):\n",
    "    # List of specific files to concatenate\n",
    "    files_to_concat = [\n",
    "        \"pg7386.txt\",\n",
    "        \"pg70049.txt\", \n",
    "        \"pg52570.txt\",\n",
    "        \"pg63311.txt\",\n",
    "        \"pg56095.txt\"\n",
    "    ]\n",
    "    \n",
    "    with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "        for filename in files_to_concat:\n",
    "            if os.path.exists(filename):\n",
    "                print(f\"Adding {filename} to {output_filename}\")\n",
    "                with open(filename, 'r', encoding='utf-8') as infile:\n",
    "                    outfile.write(infile.read())\n",
    "                    outfile.write('\\n')\n",
    "            else:\n",
    "                print(f\"Warning: {filename} not found\")\n",
    "    \n",
    "    print(f\"Files concatenated into {output_filename}\")\n",
    "\n",
    "# Run the concatenation\n",
    "file_name = \"petrie.txt\"\n",
    "concatenate_files(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344dc6b0-11dc-4586-a4a6-6ab15a8406d4",
   "metadata": {},
   "source": [
    "Double check 'petrie.txt' now to see what you've got.\n",
    "\n",
    "Hey, it's all just data, right? You mean you're worried about representation? Comprehensiveness? Balance? Nah, bro, moar data will just do the trick.\n",
    "\n",
    "Now we'll add another layer of culture, of memory, of voice on top of the 'frog dna' of the original model. If it was good enough for Jurassic Park, it's good enough for us.\n",
    "\n",
    "This next step might take a while, even with the short-trained parameters I've set below. We'll check in later. Keep an eye on the 'loss' function. When that starts to flatline (the descent is no longer steep) the model is starting to over fit. [Chantal Brousseau has a good explanation at the Programming Historian](https://programminghistorian.org/en/lessons/interrogating-national-narrative-gpt#gradient-descent-explained). (By the way, she wrote that tutorial as part of her course work here at Carleton!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108d2b60-9211-4ab3-9f64-0d41ae024639",
   "metadata": {},
   "source": [
    "# Retraining\n",
    "\n",
    "'You will know a word by the company it keeps' - all generative AI is just statistical associations of words with other words across various contexts. This is why these models are inherently conservative and uncreative; they contain within themselves enormous pressure towards the _mean_ patterns, the _average_ patterns.\n",
    "\n",
    "Remember when we took an image model, and threw away its categories and used that image model with a final bit of smooshing of our own categories to create an image classifier? We're doing something similar here. We're throwing out the last layer that says 'this word tends to go with that word in this context' and giving it Petrie's work. That last layer of data will therefore tend to guide the ultimate generation of words. Strong signals in Petrie's text are things that we can now find, using this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dbd189-f9bc-4427-82ef-502dda391d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning: this WILL TAKE QUITE SOME TIME, and the success of this will in part depend on the quality of\n",
    "# your computing hardware. If you look at the top right of this page where it says Python 3 (ipykernel) there's a circle.\n",
    "# If that circle is blacked-out, it means your machine is calculating (as does the [*] at left). Remain patient.\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.finetune(sess,\n",
    "              dataset=file_name, # this is where it grabs your data\n",
    "              model_name='124M',\n",
    "              steps=100, # in a proper training session you'd run at least 1000\n",
    "              restore_from='fresh',\n",
    "              run_name='run1',\n",
    "              print_every=10, # and you'd print out much less frequently\n",
    "              sample_every=20, # and sample every once in a while\n",
    "              save_every=50 # and probably save every 500\n",
    "              ) # with these settings and this little data, you're going to get 'overfitting' - why might that be problem? The word itself tells you...\n",
    "\n",
    "\n",
    "# optional-but-helpful parameters for gpt2.finetune:\n",
    "# restore_from: Set to fresh to start training from the base GPT-2, or set to latest to restart training from an existing checkpoint.\n",
    "# sample_every: Number of steps to print example output\n",
    "# print_every: Number of steps to print training progress.\n",
    "# learning_rate: Learning rate for the training. (default 1e-4, can lower to 1e-5 if you have <1MB input data)\n",
    "# run_name: subfolder within checkpoint to save the model. This is useful if you want to work with multiple models (will also need to specify run_name when loading the model)\n",
    "# overwrite: Set to True if you want to continue finetuning an existing model (w/ restore_from='latest') without creating duplicate copies.\n",
    "\n",
    "\n",
    "## AS THIS BLOCK RUNS it will periodically pause and generate some text given the current state of training.\n",
    "## It will print this out and you'll get a sense of how it is improving. With my default settings, it will \n",
    "## give you some stats about how well the model is training (loss, avg values; you want the loss settings to drop smoothly), \n",
    "## and then every 20 steps it'll sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eb38ed-eaaf-4c26-8ccb-5ecf611e5551",
   "metadata": {},
   "source": [
    "## Seance Time\n",
    "Ok! That took a _long_ time. Let's prompt this ghostly shade of Petrie to see what he thinks about labour issues in archaeology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d90c9ae-ff4f-4776-a6b6-e256616236fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local workers are getting a lot of attention now, and it is the first time that this is dealt with in a proper way. The main problem is that most of the local work is done by the Littors, and not by the town\n",
      "employees. It is very troublesome to have a town who cannot get all the workers together, and is\n",
      "not a suitable place for any kind of organisation.\n",
      "\n",
      "We have already noticed that many towns have been largely built on the old stock of\n",
      "trou\n",
      "====================\n",
      "Local workers are also expected to be given some work time at work, as well as a chance to work with other workers.\n",
      "\n",
      "The union will also have to be prepared to take on certain types of positions, such as inspectors.\n",
      "The position of workers should be based on the type of work done.\n",
      "\n",
      "The same is true for the workmen, who will be expected to perform the most important work.\n",
      "The union will need to be prepared to take on the most demanding positions.\n",
      "\n",
      "Work\n",
      "====================\n",
      "Local workers are going to the trouble of dealing with them, and even some of them will do so. They will tell us that the local is not a safe place, and that they should be left alone. And so they will tell us that there is no safe place, and that it is not safe for workers to go about, as they will think that it will ruin them. It is therefore necessary to have a special system of packing, in which all the workers will be left alone, and the local will\n",
      "====================\n",
      "Local workers are also required to take part in the work.\n",
      "\n",
      "The work is also carried out by the steam engine.\n",
      "The steam engine is used to speed the advance of the material,\n",
      "and to move the material.\n",
      "\n",
      "The material is placed on the line, and the advance is carried out by\n",
      "the steam.\n",
      "\n",
      "The steam is then used to move the material, and to make the speed\n",
      "of the movement, and to remove the material which is left behind.\n",
      "\n",
      "The work\n",
      "====================\n",
      "Local workers are paid for the work, and the waste is cleaned up by the government. The trade union organisation, however, has its own interests, and has to compensate the injured. The trade unions have the right to refuse, and to refuse to help the injured.\n",
      "\n",
      "The government has no right to provide food and other necessities to the people, except for the necessity of providing them for the purchase of goods. The government cannot provide for the necessities of life; it has no right to interfere with the duties\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess,\n",
    "              model_name='124M',\n",
    "              prefix=\"Local workers are\",  ## Ghostly Petrie will start with this phrase and select the most likely tokens that will follow it\n",
    "              length=100, ## Ghostly Petrie will only generate the next 100 tokens\n",
    "              temperature=0.7, ## Ghostly Petrie will choose from a wider range of probabilities; you could dial this down to 1 to see what the ur-text of his published writing might be (the absolutely most probable tokens)\n",
    "              top_p=0.9, ## another probability setting\n",
    "              nsamples=5, ## since this is all about probability, we'll do this five times.\n",
    "              batch_size=5\n",
    "              )\n",
    "\n",
    "# I've left the results of my own run below so you can get a sense of the BEHAVIOUR SPACE of this model (a concept we\n",
    "# encounter agent based modeling); when you run the cell these results will disappear and your own will turn up \n",
    "# when it finishes doing all the math!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a6e56-62e9-4540-adc8-fb22d1198e6e",
   "metadata": {},
   "source": [
    "## What do you see?\n",
    "So copy that last block of code and try exploring this ghostly echo of Petrie's writing. Write some observations down: do we learn anything about Petrie's general sensibility this way?\n",
    "\n",
    "\n",
    "## Don't Look Behind The Curtain\n",
    "EVERY so-called 'AI' is doing this under the hood; it's just the more recent ones have many more layers of modeling to constrain the probabilities of what-comes-next. It's all just mad echos bouncing around the computer, recombining in various ways.\n",
    "\n",
    "The _illusion_ of intelligence emerges from the use of a chat bot interface rather than this 'completion' interface we're using here: we're programmed to see humanity in things that look human (except for the sociopaths amongst us). \n",
    "\n",
    "These models can be further tuned by producing outputs that a human trainer (ill-paid, overworked, and exploited) judges 'more human/less human'. NOT TRUE/FALSE, not RIGHT/WRONG, but 'meh, that sounds human / doesn't sound quite as good'. The big AI models therefore are highly crafted to SOUND LIKE A PERSON: they are, quite literally, bullshit machines in the sense of that guy you hear talking out of his arsehole in a bar somewhere.\n",
    "\n",
    "Do you really want to use such things to speak for you? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
