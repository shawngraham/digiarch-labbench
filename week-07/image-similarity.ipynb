{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f01862a-fa11-48c0-91ed-756ed21681ff",
   "metadata": {},
   "source": [
    "## preliminaries\n",
    "\n",
    "Let's get the things we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe143b-733a-4d56-9118-0f13c4a06719",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install scikit-learn\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6407a1a1-5d37-47b7-bb8d-598bce66ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "# ie, everytime the computer needs to calculate a random number, it'll start by using the\n",
    "# numbers we set here and this will make all subsequent 'random' calculations the same ones\n",
    "# each time. You thought computers were actually random? Nope. They always take a seed value\n",
    "# but most of the time we don't set it, so it always appears random-enough to us. Just fyi.\n",
    "torch.manual_seed(24)\n",
    "np.random.seed(24)\n",
    "random.seed(24)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6ebc3-bd86-4792-97d2-f5664f480b13",
   "metadata": {},
   "source": [
    "# What are embeddings?\n",
    "\n",
    "Embeddings are dense vector representations of data. If we expressed geographical location as [45.232,-75.123], that's a 2 dimensional vector capturing position. Images can have thousands of dimensions when we express them as a vector. The vectors capture visual features like shapes, colors, textures. And just like places on Earth that have similar climates - eg, both Ottawa and Moscow are cold in the winter because they have a similar latitude, similar images will have similar embeddings along various vectors.\n",
    "\n",
    "We use pre-trained models because we don't have the juice to do it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361bfff1-cc74-4cc1-8c59-275fa53b0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directory\n",
    "DATA_DIR = \"data/ware\"\n",
    "\n",
    "def load_image_paths(data_dir):\n",
    "    \"\"\"Load all image paths from the data directory and its subdirectories.\"\"\"\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif'}\n",
    "    image_paths = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "# Load all image paths\n",
    "image_paths = load_image_paths(DATA_DIR)\n",
    "print(f\"Found {len(image_paths)} images in the dataset\")\n",
    "\n",
    "# Display some example images\n",
    "def display_sample_images(image_paths, n_samples=6):\n",
    "    \"\"\"Display a sample of images from the dataset.\"\"\"\n",
    "    if len(image_paths) == 0:\n",
    "        print(\"No images found in the dataset!\")\n",
    "        return\n",
    "    \n",
    "    sample_paths = random.sample(image_paths, min(n_samples, len(image_paths)))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, path in enumerate(sample_paths):\n",
    "        try:\n",
    "            img = Image.open(path)\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(os.path.basename(path))\n",
    "            axes[i].axis('off')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {path}: {e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display sample images\n",
    "display_sample_images(image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7d72b-9c0f-4424-9ff7-6e35970513e4",
   "metadata": {},
   "source": [
    "# Feature Extractor\n",
    "Now the image model we're going to use has learned already all sorts of labels. We don't want that; we just want its understanding of shapes and form. So we're going to get rid of that last layer and we'll end up with just the vectors that describe images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d721f7c-f388-45ec-be72-f77dd18cee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use ResNet-50, a popular convolutional neural network architecture.\n",
    "# We'll remove the final classification layer to get feature embeddings.\n",
    "\n",
    "class ImageFeatureExtractor:\n",
    "    def __init__(self, model_name='resnet50'):\n",
    "        \"\"\"Initialize the feature extractor with a pre-trained model.\"\"\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load pre-trained ResNet-50\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Remove the final classification layer\n",
    "        # This gives us 2048-dimensional feature vectors\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        \n",
    "        # Set to evaluation mode\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Define image preprocessing transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def extract_features(self, image_path):\n",
    "        \"\"\"Extract features from a single image.\"\"\"\n",
    "        try:\n",
    "            # Load and preprocess the image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            input_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Extract features\n",
    "            with torch.no_grad():\n",
    "                features = self.model(input_tensor)\n",
    "                \n",
    "            # Flatten the features\n",
    "            features = features.view(features.size(0), -1)\n",
    "            \n",
    "            return features.cpu().numpy().flatten()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize the feature extractor\n",
    "extractor = ImageFeatureExtractor()\n",
    "print(\"Feature extractor initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac891941-23d4-4958-990a-ed468b59543e",
   "metadata": {},
   "source": [
    "# Drop our pictures through this model\n",
    "\n",
    "Notice that the information about the image categories implied by the folder name they're in is not used in any way for turning the images into vectors. Later on we'll visualize where in the embedding space the different images fall, and we'll colour the dots by their original categories: which means that you can get a sense of how good those category images might actually be..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb31235-be4a-4e7e-a450-27402bc348b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step processes all images and extracts their feature embeddings.\n",
    "# Note: This might take a while depending on the number of images.\n",
    "\n",
    "def extract_all_features(image_paths, extractor):\n",
    "    \"\"\"Extract features from all images in the dataset.\"\"\"\n",
    "    features = []\n",
    "    valid_paths = []\n",
    "    \n",
    "    print(\"Extracting features from all images...\")\n",
    "    \n",
    "    for path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "        feature = extractor.extract_features(path)\n",
    "        if feature is not None:\n",
    "            features.append(feature)\n",
    "            valid_paths.append(path)\n",
    "    \n",
    "    features = np.array(features)\n",
    "    print(f\"Successfully extracted features from {len(features)} images\")\n",
    "    print(f\"Feature shape: {features.shape}\")\n",
    "    \n",
    "    return features, valid_paths\n",
    "\n",
    "# Extract features (this might take a few minutes)\n",
    "if len(image_paths) > 0:\n",
    "    features, valid_image_paths = extract_all_features(image_paths, extractor)\n",
    "else:\n",
    "    print(\"No images found to process!\")\n",
    "    features, valid_image_paths = np.array([]), []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb6d8a-20e9-4221-8759-c45d4909b981",
   "metadata": {},
   "source": [
    "# Calculate Similarity!\n",
    "\n",
    "Because vectors are directions, we can work out similarity by doing some geometry on them. In this case we'll use cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df1ddf7-0715-47dd-a37f-fb4239d391ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll compute similarity between images using cosine similarity.\n",
    "# Cosine similarity measures the cosine of the angle between two vectors.\n",
    "def find_similar_images(query_idx, features, valid_paths, top_k=5):\n",
    "    \"\"\"Find the most similar images to a query image.\"\"\"\n",
    "    if len(features) == 0:\n",
    "        return [], []\n",
    "    \n",
    "    # Compute cosine similarity between query and all images\n",
    "    query_features = features[query_idx:query_idx+1]\n",
    "    similarities = cosine_similarity(query_features, features).flatten()\n",
    "    \n",
    "    # Get top-k most similar images (excluding the query itself)\n",
    "    similar_indices = np.argsort(similarities)[::-1]\n",
    "    \n",
    "    # Remove the query image itself from results\n",
    "    similar_indices = similar_indices[similar_indices != query_idx][:top_k]\n",
    "    \n",
    "    return similar_indices, similarities[similar_indices]\n",
    "\n",
    "def display_similar_images(query_idx, similar_indices, similarities, valid_paths):\n",
    "    \"\"\"Display the query image and its most similar images.\"\"\"\n",
    "    if len(valid_paths) == 0:\n",
    "        print(\"No valid images to display!\")\n",
    "        return\n",
    "    \n",
    "    n_images = len(similar_indices) + 1\n",
    "    fig, axes = plt.subplots(1, n_images, figsize=(15, 3))\n",
    "    \n",
    "    if n_images == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Display query image\n",
    "    query_img = Image.open(valid_paths[query_idx])\n",
    "    axes[0].imshow(query_img)\n",
    "    axes[0].set_title(f\"Query Image\\n{os.path.basename(valid_paths[query_idx])}\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Display similar images\n",
    "    for i, (idx, sim) in enumerate(zip(similar_indices, similarities)):\n",
    "        similar_img = Image.open(valid_paths[idx])\n",
    "        axes[i+1].imshow(similar_img)\n",
    "        axes[i+1].set_title(f\"Similarity: {sim:.3f}\\n{os.path.basename(valid_paths[idx])}\")\n",
    "        axes[i+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b5437a-8943-424a-9b71-fb277d6b912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar images to a random query\n",
    "if len(features) > 0:\n",
    "    query_idx = random.randint(0, len(features) - 1)\n",
    "    similar_indices, similarities = find_similar_images(query_idx, features, valid_image_paths)\n",
    "    \n",
    "    print(f\"Finding images similar to: {os.path.basename(valid_image_paths[query_idx])}\")\n",
    "    display_similar_images(query_idx, similar_indices, similarities, valid_image_paths)\n",
    "else:\n",
    "    print(\"No features available for similarity computation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7615b8-bdef-4285-bbe0-cb0d10122fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def another_similarity_search(features, valid_paths, n_queries=3):\n",
    "    \"\"\"Demonstrate similarity search with multiple random queries.\"\"\"\n",
    "    if len(features) == 0:\n",
    "        print(\"No features available for similarity search!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Demonstrating similarity search with {n_queries} random queries...\")\n",
    "    \n",
    "    for i in range(n_queries):\n",
    "        print(f\"\\n--- Query {i+1} ---\")\n",
    "        query_idx = random.randint(0, len(features) - 1)\n",
    "        similar_indices, similarities = find_similar_images(query_idx, features, valid_paths, top_k=4)\n",
    "        display_similar_images(query_idx, similar_indices, similarities, valid_paths)\n",
    "\n",
    "# Run another similarity search\n",
    "if len(features) > 0:\n",
    "    another_similarity_search(features, valid_image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea3944d-3adf-4038-bc4e-3fa2a0ad5cd9",
   "metadata": {},
   "source": [
    "# Visualize Similarity for ALL the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbadac2-d39f-442a-b6a5-a1fe9297421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-dimensional embeddings are hard to visualize directly.\n",
    "# We'll use dimensionality reduction techniques to project them into 2D space.\n",
    "\n",
    "def visualize_embeddings_2d(features, valid_paths, method='tsne', n_samples=None, \n",
    "                           show_labels=False):\n",
    "    \"\"\"\n",
    "    Visualize embeddings in 2D space using PCA or t-SNE.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    features : numpy.ndarray\n",
    "        Feature embeddings for all images\n",
    "    valid_paths : list\n",
    "        List of valid image paths\n",
    "    method : str\n",
    "        Dimensionality reduction method ('pca' or 'tsne')\n",
    "    n_samples : int, optional\n",
    "        Number of samples to visualize (None for all)\n",
    "    show_labels : bool\n",
    "        Whether to show filename labels on the plot\n",
    "    \"\"\"\n",
    "    if len(features) == 0:\n",
    "        print(\"No features available for visualization!\")\n",
    "        return\n",
    "    \n",
    "    # Limit number of samples for faster computation\n",
    "    if n_samples and len(features) > n_samples:\n",
    "        indices = random.sample(range(len(features)), n_samples)\n",
    "        features_subset = features[indices]\n",
    "        paths_subset = [valid_paths[i] for i in indices]\n",
    "    else:\n",
    "        features_subset = features\n",
    "        paths_subset = valid_paths\n",
    "    \n",
    "    print(f\"Visualizing {len(features_subset)} images using {method.upper()}...\")\n",
    "    \n",
    "    if method == 'pca':\n",
    "        # Principal Component Analysis\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "        embeddings_2d = reducer.fit_transform(features_subset)\n",
    "        title = f\"Image Embeddings Visualization (PCA)\\nExplained Variance: {reducer.explained_variance_ratio_.sum():.2%}\"\n",
    "    \n",
    "    elif method == 'tsne':\n",
    "        # t-Distributed Stochastic Neighbor Embedding\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features_subset)-1))\n",
    "        embeddings_2d = reducer.fit_transform(features_subset)\n",
    "        title = \"Image Embeddings Visualization (t-SNE)\"\n",
    "    \n",
    "    # Create the visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Color points by their directory (if available)\n",
    "    colors = []\n",
    "    labels = []\n",
    "    for path in paths_subset:\n",
    "        # Extract directory name as label\n",
    "        parent_dir = os.path.basename(os.path.dirname(path))\n",
    "        if parent_dir not in labels:\n",
    "            labels.append(parent_dir)\n",
    "        colors.append(labels.index(parent_dir))\n",
    "    \n",
    "    # Create scatter plot\n",
    "    scatter = ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                       c=colors, cmap='tab10', alpha=0.7, s=50)\n",
    "    \n",
    "    # Add filename labels if requested\n",
    "    if show_labels:\n",
    "        for i, (x, y, path) in enumerate(zip(embeddings_2d[:, 0], embeddings_2d[:, 1], paths_subset)):\n",
    "            filename = os.path.basename(path)\n",
    "            # Truncate long filenames\n",
    "            if len(filename) > 15:\n",
    "                filename = filename[:12] + '...'\n",
    "            ax.annotate(filename, (x, y), xytext=(5, 5), textcoords='offset points',\n",
    "                       fontsize=8, alpha=0.7, ha='left')\n",
    "    \n",
    "    # Add colorbar if we have multiple categories\n",
    "    # where each color corresponds to the category\n",
    "    if len(labels) > 1:\n",
    "        plt.colorbar(scatter, label='Directory')\n",
    "    \n",
    "    # Add legend if we have multiple categories\n",
    "    if len(labels) > 1 and len(labels) <= 10:\n",
    "        handles = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                            markerfacecolor=plt.cm.tab10(i/len(labels)), \n",
    "                            markersize=8, label=label) \n",
    "                  for i, label in enumerate(labels)]\n",
    "        ax.legend(handles=handles, title='Directory', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return embeddings_2d\n",
    "\n",
    "# Visualize embeddings using both PCA and t-SNE\n",
    "if len(features) > 0:\n",
    "    print(\"Creating 2D visualizations of the image embeddings...\")\n",
    "    \n",
    "    # Standard PCA visualization\n",
    "    pca_embeddings = visualize_embeddings_2d(features, valid_image_paths, method='pca', n_samples=200)\n",
    "    \n",
    "    # t-SNE visualization with filename labels\n",
    "    print(\"\\nCreating t-SNE visualization with filename labels...\")\n",
    "    tsne_embeddings = visualize_embeddings_2d(features, valid_image_paths, method='tsne', \n",
    "                                             n_samples=50, show_labels=True)\n",
    "else:\n",
    "    print(\"No features available for visualization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9f749-f48f-4487-a681-26ea3923b0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze the distribution of similarities and feature statistics.\n",
    "\n",
    "def analyze_embeddings(features, valid_paths):\n",
    "    \"\"\"Analyze the properties of extracted embeddings.\"\"\"\n",
    "    if len(features) == 0:\n",
    "        print(\"No features available for analysis!\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== Embedding Analysis ===\")\n",
    "    print(f\"Number of images: {len(features)}\")\n",
    "    print(f\"Feature dimensionality: {features.shape[1]}\")\n",
    "    print(f\"Feature range: [{features.min():.3f}, {features.max():.3f}]\")\n",
    "    print(f\"Mean feature magnitude: {np.linalg.norm(features, axis=1).mean():.3f}\")\n",
    "    \n",
    "    # Compute pairwise similarities\n",
    "    print(\"\\nComputing pairwise similarities...\")\n",
    "    similarities = cosine_similarity(features)\n",
    "    \n",
    "    # Remove diagonal (self-similarities)\n",
    "    np.fill_diagonal(similarities, 0)\n",
    "    \n",
    "    # Analyze similarity distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histogram of similarities\n",
    "    axes[0].hist(similarities.flatten(), bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_title('Distribution of Pairwise Similarities')\n",
    "    axes[0].set_xlabel('Cosine Similarity')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Feature magnitude distribution\n",
    "    feature_magnitudes = np.linalg.norm(features, axis=1)\n",
    "    axes[1].hist(feature_magnitudes, bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_title('Distribution of Feature Magnitudes')\n",
    "    axes[1].set_xlabel('L2 Norm')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Average pairwise similarity: {similarities.mean():.3f}\")\n",
    "    print(f\"Std of pairwise similarities: {similarities.std():.3f}\")\n",
    "    print(f\"Most similar pair similarity: {similarities.max():.3f}\")\n",
    "\n",
    "# Run analysis\n",
    "if len(features) > 0:\n",
    "    analyze_embeddings(features, valid_image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525ede6d-158d-46d7-9494-aa57ed2d6a67",
   "metadata": {},
   "source": [
    "Cosine similarity ranges from -1 to +1, where 1 = identical, 0 = orthogonal, -1 = opposite\n",
    "+ If average pairwise similarity is high, then the images are sharing similar kinds of composition, colours, visual organization etc.\n",
    "+ The standard deviation (std) is low, then this suggests that there is consistent similarity, limited diversity, tight clustering, or a homogeneous dataset.\n",
    "+ If there is a most similar pair scoring 1.000, then that suggests that there's a duplicate image in the dataset somewhere!\n",
    "\n",
    "So... what does this all imply about our dataset? Make a note!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
